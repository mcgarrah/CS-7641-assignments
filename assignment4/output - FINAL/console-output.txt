no reward - all

C:\Users\mcgarrah\.conda\python35\python.exe C:/Users/mcgarrah/CS-7641-assignments/assignment4/run_experiment.py --all --verbose
2019-04-11 23:34:03,197 - __main__ - INFO - Using seed 0
2019-04-11 23:34:03,197 - __main__ - INFO - Creating MDPs
2019-04-11 23:34:03,197 - __main__ - INFO - ----------
2019-04-11 23:34:03,210 - __main__ - INFO - Mazeworld (4x4): State space: 16, Action space: 4
2019-04-11 23:34:03,210 - __main__ - INFO - Mazeworld (5x5): State space: 25, Action space: 4
2019-04-11 23:34:03,210 - __main__ - INFO - Mazeworld (11x11): State space: 121, Action space: 4
2019-04-11 23:34:03,210 - __main__ - INFO - Mazeworld (8x8): State space: 64, Action space: 4
2019-04-11 23:34:03,210 - __main__ - INFO - Mazeworld (9x9): State space: 81, Action space: 4
2019-04-11 23:34:03,210 - __main__ - INFO - Mazeworld (15x15): State space: 225, Action space: 4
2019-04-11 23:34:03,210 - __main__ - INFO - ----------
2019-04-11 23:34:03,210 - __main__ - INFO - Running experiments
2019-04-11 23:34:03,211 - __main__ - INFO - Running PI experiment: Mazeworld (4x4)
2019-04-11 23:34:03,216 - experiments.base - INFO - Searching PI in 10 dimensions
2019-04-11 23:34:03,216 - experiments.base - INFO - 1/10 Processing PI with discount factor 0.0
2019-04-11 23:34:03,217 - experiments.base - INFO - Took 2 steps
2019-04-11 23:34:04,884 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:34:04,891 - experiments.base - INFO - 2/10 Processing PI with discount factor 0.1
2019-04-11 23:34:04,895 - experiments.base - INFO - Took 2 steps
2019-04-11 23:34:05,276 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:34:05,285 - experiments.base - INFO - 3/10 Processing PI with discount factor 0.2
2019-04-11 23:34:05,289 - experiments.base - INFO - Took 2 steps
2019-04-11 23:34:05,657 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:34:05,667 - experiments.base - INFO - 4/10 Processing PI with discount factor 0.3
2019-04-11 23:34:05,671 - experiments.base - INFO - Took 2 steps
2019-04-11 23:34:06,036 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:34:06,046 - experiments.base - INFO - 5/10 Processing PI with discount factor 0.4
2019-04-11 23:34:06,051 - experiments.base - INFO - Took 3 steps
2019-04-11 23:34:06,417 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:34:06,426 - experiments.base - INFO - 6/10 Processing PI with discount factor 0.5
2019-04-11 23:34:06,430 - experiments.base - INFO - Took 2 steps
2019-04-11 23:34:06,828 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:34:06,838 - experiments.base - INFO - 7/10 Processing PI with discount factor 0.6
2019-04-11 23:34:06,842 - experiments.base - INFO - Took 2 steps
2019-04-11 23:34:07,211 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:34:07,223 - experiments.base - INFO - 8/10 Processing PI with discount factor 0.7
2019-04-11 23:34:07,229 - experiments.base - INFO - Took 2 steps
2019-04-11 23:34:07,592 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:34:07,601 - experiments.base - INFO - 9/10 Processing PI with discount factor 0.8
2019-04-11 23:34:07,608 - experiments.base - INFO - Took 2 steps
2019-04-11 23:34:07,963 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:34:07,973 - experiments.base - INFO - 10/10 Processing PI with discount factor 0.9
2019-04-11 23:34:07,984 - experiments.base - INFO - Took 2 steps
2019-04-11 23:34:08,355 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:34:08,365 - __main__ - INFO - Running PI experiment: Mazeworld (5x5)
2019-04-11 23:34:08,365 - experiments.base - INFO - Searching PI in 10 dimensions
2019-04-11 23:34:08,365 - experiments.base - INFO - 1/10 Processing PI with discount factor 0.0
2019-04-11 23:34:08,368 - experiments.base - INFO - Took 2 steps
2019-04-11 23:34:09,943 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:34:09,950 - experiments.base - INFO - 2/10 Processing PI with discount factor 0.1
2019-04-11 23:34:09,961 - experiments.base - INFO - Took 4 steps
2019-04-11 23:34:10,377 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:34:10,387 - experiments.base - INFO - 3/10 Processing PI with discount factor 0.2
2019-04-11 23:34:10,398 - experiments.base - INFO - Took 4 steps
2019-04-11 23:34:10,825 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:34:10,832 - experiments.base - INFO - 4/10 Processing PI with discount factor 0.3
2019-04-11 23:34:10,839 - experiments.base - INFO - Took 3 steps
2019-04-11 23:34:11,247 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:34:11,253 - experiments.base - INFO - 5/10 Processing PI with discount factor 0.4
2019-04-11 23:34:11,259 - experiments.base - INFO - Took 2 steps
2019-04-11 23:34:11,673 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:34:11,680 - experiments.base - INFO - 6/10 Processing PI with discount factor 0.5
2019-04-11 23:34:11,687 - experiments.base - INFO - Took 2 steps
2019-04-11 23:34:12,089 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:34:12,095 - experiments.base - INFO - 7/10 Processing PI with discount factor 0.6
2019-04-11 23:34:12,102 - experiments.base - INFO - Took 2 steps
2019-04-11 23:34:12,575 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:34:12,582 - experiments.base - INFO - 8/10 Processing PI with discount factor 0.7
2019-04-11 23:34:12,594 - experiments.base - INFO - Took 2 steps
2019-04-11 23:34:13,000 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:34:13,006 - experiments.base - INFO - 9/10 Processing PI with discount factor 0.8
2019-04-11 23:34:13,016 - experiments.base - INFO - Took 2 steps
2019-04-11 23:34:13,424 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:34:13,430 - experiments.base - INFO - 10/10 Processing PI with discount factor 0.9
2019-04-11 23:34:13,447 - experiments.base - INFO - Took 2 steps
2019-04-11 23:34:13,861 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:34:13,868 - __main__ - INFO - Running PI experiment: Mazeworld (11x11)
2019-04-11 23:34:13,868 - experiments.base - INFO - Searching PI in 10 dimensions
2019-04-11 23:34:13,869 - experiments.base - INFO - 1/10 Processing PI with discount factor 0.0
2019-04-11 23:34:13,881 - experiments.base - INFO - Took 2 steps
2019-04-11 23:34:16,112 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:34:16,118 - experiments.base - INFO - 2/10 Processing PI with discount factor 0.1
2019-04-11 23:34:16,170 - experiments.base - INFO - Took 4 steps
2019-04-11 23:34:18,358 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:34:18,365 - experiments.base - INFO - 3/10 Processing PI with discount factor 0.2
2019-04-11 23:34:18,463 - experiments.base - INFO - Took 7 steps
2019-04-11 23:34:20,664 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:34:20,673 - experiments.base - INFO - 4/10 Processing PI with discount factor 0.3
2019-04-11 23:34:20,755 - experiments.base - INFO - Took 5 steps
2019-04-11 23:34:23,032 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:34:23,039 - experiments.base - INFO - 5/10 Processing PI with discount factor 0.4
2019-04-11 23:34:23,125 - experiments.base - INFO - Took 5 steps
2019-04-11 23:34:25,303 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:34:25,311 - experiments.base - INFO - 6/10 Processing PI with discount factor 0.5
2019-04-11 23:34:25,440 - experiments.base - INFO - Took 6 steps
2019-04-11 23:34:27,631 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:34:27,637 - experiments.base - INFO - 7/10 Processing PI with discount factor 0.6
2019-04-11 23:34:27,867 - experiments.base - INFO - Took 9 steps
2019-04-11 23:34:30,134 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:34:30,144 - experiments.base - INFO - 8/10 Processing PI with discount factor 0.7
2019-04-11 23:34:30,368 - experiments.base - INFO - Took 7 steps
2019-04-11 23:34:31,454 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-11 23:34:31,466 - experiments.base - INFO - 9/10 Processing PI with discount factor 0.8
2019-04-11 23:34:31,573 - experiments.base - INFO - Took 3 steps
2019-04-11 23:34:32,651 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-11 23:34:32,663 - experiments.base - INFO - 10/10 Processing PI with discount factor 0.9
2019-04-11 23:34:32,799 - experiments.base - INFO - Took 3 steps
2019-04-11 23:34:33,934 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-11 23:34:33,946 - __main__ - INFO - Running PI experiment: Mazeworld (8x8)
2019-04-11 23:34:33,947 - experiments.base - INFO - Searching PI in 10 dimensions
2019-04-11 23:34:33,947 - experiments.base - INFO - 1/10 Processing PI with discount factor 0.0
2019-04-11 23:34:33,954 - experiments.base - INFO - Took 2 steps
2019-04-11 23:34:35,823 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:34:35,831 - experiments.base - INFO - 2/10 Processing PI with discount factor 0.1
2019-04-11 23:34:35,867 - experiments.base - INFO - Took 6 steps
2019-04-11 23:34:37,757 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:34:37,765 - experiments.base - INFO - 3/10 Processing PI with discount factor 0.2
2019-04-11 23:34:37,809 - experiments.base - INFO - Took 6 steps
2019-04-11 23:34:39,665 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:34:39,674 - experiments.base - INFO - 4/10 Processing PI with discount factor 0.3
2019-04-11 23:34:39,717 - experiments.base - INFO - Took 5 steps
2019-04-11 23:34:41,555 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:34:41,561 - experiments.base - INFO - 5/10 Processing PI with discount factor 0.4
2019-04-11 23:34:41,611 - experiments.base - INFO - Took 5 steps
2019-04-11 23:34:42,361 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-11 23:34:42,371 - experiments.base - INFO - 6/10 Processing PI with discount factor 0.5
2019-04-11 23:34:42,404 - experiments.base - INFO - Took 3 steps
2019-04-11 23:34:43,112 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-11 23:34:43,122 - experiments.base - INFO - 7/10 Processing PI with discount factor 0.6
2019-04-11 23:34:43,160 - experiments.base - INFO - Took 3 steps
2019-04-11 23:34:43,868 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-11 23:34:43,880 - experiments.base - INFO - 8/10 Processing PI with discount factor 0.7
2019-04-11 23:34:43,917 - experiments.base - INFO - Took 3 steps
2019-04-11 23:34:44,665 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-11 23:34:44,677 - experiments.base - INFO - 9/10 Processing PI with discount factor 0.8
2019-04-11 23:34:44,721 - experiments.base - INFO - Took 3 steps
2019-04-11 23:34:45,431 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-11 23:34:45,443 - experiments.base - INFO - 10/10 Processing PI with discount factor 0.9
2019-04-11 23:34:45,500 - experiments.base - INFO - Took 3 steps
2019-04-11 23:34:46,220 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-11 23:34:46,233 - __main__ - INFO - Running PI experiment: Mazeworld (9x9)
2019-04-11 23:34:46,236 - experiments.base - INFO - Searching PI in 10 dimensions
2019-04-11 23:34:46,236 - experiments.base - INFO - 1/10 Processing PI with discount factor 0.0
2019-04-11 23:34:46,243 - experiments.base - INFO - Took 2 steps
2019-04-11 23:34:48,203 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:34:48,210 - experiments.base - INFO - 2/10 Processing PI with discount factor 0.1
2019-04-11 23:34:48,242 - experiments.base - INFO - Took 4 steps
2019-04-11 23:34:50,187 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:34:50,194 - experiments.base - INFO - 3/10 Processing PI with discount factor 0.2
2019-04-11 23:34:50,243 - experiments.base - INFO - Took 5 steps
2019-04-11 23:34:52,246 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:34:52,253 - experiments.base - INFO - 4/10 Processing PI with discount factor 0.3
2019-04-11 23:34:52,306 - experiments.base - INFO - Took 5 steps
2019-04-11 23:34:54,246 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:34:54,253 - experiments.base - INFO - 5/10 Processing PI with discount factor 0.4
2019-04-11 23:34:54,298 - experiments.base - INFO - Took 4 steps
2019-04-11 23:34:56,236 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:34:56,243 - experiments.base - INFO - 6/10 Processing PI with discount factor 0.5
2019-04-11 23:34:56,267 - experiments.base - INFO - Took 2 steps
2019-04-11 23:34:58,273 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:34:58,280 - experiments.base - INFO - 7/10 Processing PI with discount factor 0.6
2019-04-11 23:34:58,339 - experiments.base - INFO - Took 4 steps
2019-04-11 23:35:00,292 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:35:00,299 - experiments.base - INFO - 8/10 Processing PI with discount factor 0.7
2019-04-11 23:35:00,421 - experiments.base - INFO - Took 6 steps
2019-04-11 23:35:02,365 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:35:02,372 - experiments.base - INFO - 9/10 Processing PI with discount factor 0.8
2019-04-11 23:35:02,536 - experiments.base - INFO - Took 6 steps
2019-04-11 23:35:03,427 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-11 23:35:03,464 - experiments.base - INFO - 10/10 Processing PI with discount factor 0.9
2019-04-11 23:35:03,539 - experiments.base - INFO - Took 2 steps
2019-04-11 23:35:04,388 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-11 23:35:04,424 - __main__ - INFO - Running PI experiment: Mazeworld (15x15)
2019-04-11 23:35:04,424 - experiments.base - INFO - Searching PI in 10 dimensions
2019-04-11 23:35:04,424 - experiments.base - INFO - 1/10 Processing PI with discount factor 0.0
2019-04-11 23:35:04,447 - experiments.base - INFO - Took 2 steps
2019-04-11 23:35:07,387 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:35:07,394 - experiments.base - INFO - 2/10 Processing PI with discount factor 0.1
2019-04-11 23:35:07,476 - experiments.base - INFO - Took 4 steps
2019-04-11 23:35:10,341 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:35:10,348 - experiments.base - INFO - 3/10 Processing PI with discount factor 0.2
2019-04-11 23:35:10,470 - experiments.base - INFO - Took 5 steps
2019-04-11 23:35:13,398 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:35:13,407 - experiments.base - INFO - 4/10 Processing PI with discount factor 0.3
2019-04-11 23:35:13,552 - experiments.base - INFO - Took 5 steps
2019-04-11 23:35:16,459 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:35:16,466 - experiments.base - INFO - 5/10 Processing PI with discount factor 0.4
2019-04-11 23:35:16,706 - experiments.base - INFO - Took 7 steps
2019-04-11 23:35:19,601 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:35:19,608 - experiments.base - INFO - 6/10 Processing PI with discount factor 0.5
2019-04-11 23:35:20,237 - experiments.base - INFO - Took 14 steps
2019-04-11 23:35:23,230 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:35:23,236 - experiments.base - INFO - 7/10 Processing PI with discount factor 0.6
2019-04-11 23:35:24,176 - experiments.base - INFO - Took 17 steps
2019-04-11 23:35:27,121 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:35:27,128 - experiments.base - INFO - 8/10 Processing PI with discount factor 0.7
2019-04-11 23:35:28,184 - experiments.base - INFO - Took 16 steps
2019-04-11 23:35:31,165 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:35:31,173 - experiments.base - INFO - 9/10 Processing PI with discount factor 0.8
2019-04-11 23:35:31,720 - experiments.base - INFO - Took 7 steps
2019-04-11 23:35:33,526 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-11 23:35:33,536 - experiments.base - INFO - 10/10 Processing PI with discount factor 0.9
2019-04-11 23:35:33,749 - experiments.base - INFO - Took 2 steps
2019-04-11 23:35:35,614 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-11 23:35:35,621 - __main__ - INFO - Running VI experiment: Mazeworld (4x4)
2019-04-11 23:35:35,622 - experiments.base - INFO - Searching VI in 10 dimensions
2019-04-11 23:35:35,624 - experiments.base - INFO - 1/10 Processing VI with discount factor 0.0
2019-04-11 23:35:35,625 - experiments.base - INFO - Took 2 steps
2019-04-11 23:35:37,128 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:35:37,137 - experiments.base - INFO - 2/10 Processing VI with discount factor 0.1
2019-04-11 23:35:37,141 - experiments.base - INFO - Took 7 steps
2019-04-11 23:35:37,510 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:35:37,520 - experiments.base - INFO - 3/10 Processing VI with discount factor 0.2
2019-04-11 23:35:37,525 - experiments.base - INFO - Took 7 steps
2019-04-11 23:35:37,910 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:35:37,920 - experiments.base - INFO - 4/10 Processing VI with discount factor 0.3
2019-04-11 23:35:37,924 - experiments.base - INFO - Took 7 steps
2019-04-11 23:35:38,305 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:35:38,316 - experiments.base - INFO - 5/10 Processing VI with discount factor 0.4
2019-04-11 23:35:38,319 - experiments.base - INFO - Took 7 steps
2019-04-11 23:35:38,697 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:35:38,709 - experiments.base - INFO - 6/10 Processing VI with discount factor 0.5
2019-04-11 23:35:38,711 - experiments.base - INFO - Took 7 steps
2019-04-11 23:35:39,130 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:35:39,141 - experiments.base - INFO - 7/10 Processing VI with discount factor 0.6
2019-04-11 23:35:39,144 - experiments.base - INFO - Took 7 steps
2019-04-11 23:35:39,522 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:35:39,532 - experiments.base - INFO - 8/10 Processing VI with discount factor 0.7
2019-04-11 23:35:39,536 - experiments.base - INFO - Took 7 steps
2019-04-11 23:35:39,914 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:35:39,926 - experiments.base - INFO - 9/10 Processing VI with discount factor 0.8
2019-04-11 23:35:39,928 - experiments.base - INFO - Took 7 steps
2019-04-11 23:35:40,308 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:35:40,316 - experiments.base - INFO - 10/10 Processing VI with discount factor 0.9
2019-04-11 23:35:40,319 - experiments.base - INFO - Took 7 steps
2019-04-11 23:35:40,698 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:35:40,709 - __main__ - INFO - Running VI experiment: Mazeworld (5x5)
2019-04-11 23:35:40,710 - experiments.base - INFO - Searching VI in 10 dimensions
2019-04-11 23:35:40,710 - experiments.base - INFO - 1/10 Processing VI with discount factor 0.0
2019-04-11 23:35:40,713 - experiments.base - INFO - Took 2 steps
2019-04-11 23:35:42,299 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:35:42,305 - experiments.base - INFO - 2/10 Processing VI with discount factor 0.1
2019-04-11 23:35:42,312 - experiments.base - INFO - Took 7 steps
2019-04-11 23:35:42,737 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:35:42,744 - experiments.base - INFO - 3/10 Processing VI with discount factor 0.2
2019-04-11 23:35:42,753 - experiments.base - INFO - Took 9 steps
2019-04-11 23:35:43,197 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:35:43,204 - experiments.base - INFO - 4/10 Processing VI with discount factor 0.3
2019-04-11 23:35:43,211 - experiments.base - INFO - Took 9 steps
2019-04-11 23:35:43,654 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:35:43,661 - experiments.base - INFO - 5/10 Processing VI with discount factor 0.4
2019-04-11 23:35:43,670 - experiments.base - INFO - Took 9 steps
2019-04-11 23:35:44,104 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:35:44,111 - experiments.base - INFO - 6/10 Processing VI with discount factor 0.5
2019-04-11 23:35:44,118 - experiments.base - INFO - Took 9 steps
2019-04-11 23:35:44,552 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:35:44,559 - experiments.base - INFO - 7/10 Processing VI with discount factor 0.6
2019-04-11 23:35:44,566 - experiments.base - INFO - Took 9 steps
2019-04-11 23:35:45,052 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:35:45,059 - experiments.base - INFO - 8/10 Processing VI with discount factor 0.7
2019-04-11 23:35:45,065 - experiments.base - INFO - Took 9 steps
2019-04-11 23:35:45,494 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:35:45,502 - experiments.base - INFO - 9/10 Processing VI with discount factor 0.8
2019-04-11 23:35:45,507 - experiments.base - INFO - Took 9 steps
2019-04-11 23:35:45,953 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:35:45,959 - experiments.base - INFO - 10/10 Processing VI with discount factor 0.9
2019-04-11 23:35:45,967 - experiments.base - INFO - Took 9 steps
2019-04-11 23:35:46,410 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:35:46,417 - __main__ - INFO - Running VI experiment: Mazeworld (11x11)
2019-04-11 23:35:46,420 - experiments.base - INFO - Searching VI in 10 dimensions
2019-04-11 23:35:46,420 - experiments.base - INFO - 1/10 Processing VI with discount factor 0.0
2019-04-11 23:35:46,427 - experiments.base - INFO - Took 2 steps
2019-04-11 23:35:48,645 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:35:48,651 - experiments.base - INFO - 2/10 Processing VI with discount factor 0.1
2019-04-11 23:35:48,676 - experiments.base - INFO - Took 7 steps
2019-04-11 23:35:50,882 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:35:50,890 - experiments.base - INFO - 3/10 Processing VI with discount factor 0.2
2019-04-11 23:35:50,920 - experiments.base - INFO - Took 9 steps
2019-04-11 23:35:53,151 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:35:53,161 - experiments.base - INFO - 4/10 Processing VI with discount factor 0.3
2019-04-11 23:35:53,200 - experiments.base - INFO - Took 11 steps
2019-04-11 23:35:55,457 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:35:55,463 - experiments.base - INFO - 5/10 Processing VI with discount factor 0.4
2019-04-11 23:35:55,503 - experiments.base - INFO - Took 12 steps
2019-04-11 23:35:57,756 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:35:57,763 - experiments.base - INFO - 6/10 Processing VI with discount factor 0.5
2019-04-11 23:35:57,816 - experiments.base - INFO - Took 16 steps
2019-04-11 23:36:00,049 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:00,056 - experiments.base - INFO - 7/10 Processing VI with discount factor 0.6
2019-04-11 23:36:00,121 - experiments.base - INFO - Took 19 steps
2019-04-11 23:36:02,421 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:02,433 - experiments.base - INFO - 8/10 Processing VI with discount factor 0.7
2019-04-11 23:36:02,516 - experiments.base - INFO - Took 26 steps
2019-04-11 23:36:03,686 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-11 23:36:03,697 - experiments.base - INFO - 9/10 Processing VI with discount factor 0.8
2019-04-11 23:36:03,783 - experiments.base - INFO - Took 26 steps
2019-04-11 23:36:04,973 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-11 23:36:04,983 - experiments.base - INFO - 10/10 Processing VI with discount factor 0.9
2019-04-11 23:36:05,069 - experiments.base - INFO - Took 26 steps
2019-04-11 23:36:06,313 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-11 23:36:06,325 - __main__ - INFO - Running VI experiment: Mazeworld (8x8)
2019-04-11 23:36:06,326 - experiments.base - INFO - Searching VI in 10 dimensions
2019-04-11 23:36:06,328 - experiments.base - INFO - 1/10 Processing VI with discount factor 0.0
2019-04-11 23:36:06,331 - experiments.base - INFO - Took 2 steps
2019-04-11 23:36:08,151 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:08,158 - experiments.base - INFO - 2/10 Processing VI with discount factor 0.1
2019-04-11 23:36:08,174 - experiments.base - INFO - Took 7 steps
2019-04-11 23:36:10,059 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:10,065 - experiments.base - INFO - 3/10 Processing VI with discount factor 0.2
2019-04-11 23:36:10,082 - experiments.base - INFO - Took 9 steps
2019-04-11 23:36:11,954 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:11,969 - experiments.base - INFO - 4/10 Processing VI with discount factor 0.3
2019-04-11 23:36:11,989 - experiments.base - INFO - Took 11 steps
2019-04-11 23:36:13,894 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:13,901 - experiments.base - INFO - 5/10 Processing VI with discount factor 0.4
2019-04-11 23:36:13,928 - experiments.base - INFO - Took 14 steps
2019-04-11 23:36:14,765 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-11 23:36:14,776 - experiments.base - INFO - 6/10 Processing VI with discount factor 0.5
2019-04-11 23:36:14,802 - experiments.base - INFO - Took 15 steps
2019-04-11 23:36:15,562 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-11 23:36:15,572 - experiments.base - INFO - 7/10 Processing VI with discount factor 0.6
2019-04-11 23:36:15,599 - experiments.base - INFO - Took 15 steps
2019-04-11 23:36:16,367 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-11 23:36:16,378 - experiments.base - INFO - 8/10 Processing VI with discount factor 0.7
2019-04-11 23:36:16,410 - experiments.base - INFO - Took 15 steps
2019-04-11 23:36:17,213 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-11 23:36:17,223 - experiments.base - INFO - 9/10 Processing VI with discount factor 0.8
2019-04-11 23:36:17,250 - experiments.base - INFO - Took 15 steps
2019-04-11 23:36:18,012 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-11 23:36:18,023 - experiments.base - INFO - 10/10 Processing VI with discount factor 0.9
2019-04-11 23:36:18,048 - experiments.base - INFO - Took 15 steps
2019-04-11 23:36:18,813 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-11 23:36:18,826 - __main__ - INFO - Running VI experiment: Mazeworld (9x9)
2019-04-11 23:36:18,828 - experiments.base - INFO - Searching VI in 10 dimensions
2019-04-11 23:36:18,828 - experiments.base - INFO - 1/10 Processing VI with discount factor 0.0
2019-04-11 23:36:18,834 - experiments.base - INFO - Took 2 steps
2019-04-11 23:36:20,825 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:20,831 - experiments.base - INFO - 2/10 Processing VI with discount factor 0.1
2019-04-11 23:36:20,846 - experiments.base - INFO - Took 7 steps
2019-04-11 23:36:22,825 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:22,832 - experiments.base - INFO - 3/10 Processing VI with discount factor 0.2
2019-04-11 23:36:22,852 - experiments.base - INFO - Took 9 steps
2019-04-11 23:36:24,835 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:24,842 - experiments.base - INFO - 4/10 Processing VI with discount factor 0.3
2019-04-11 23:36:24,869 - experiments.base - INFO - Took 11 steps
2019-04-11 23:36:26,846 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:26,852 - experiments.base - INFO - 5/10 Processing VI with discount factor 0.4
2019-04-11 23:36:26,878 - experiments.base - INFO - Took 11 steps
2019-04-11 23:36:28,851 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:28,858 - experiments.base - INFO - 6/10 Processing VI with discount factor 0.5
2019-04-11 23:36:28,882 - experiments.base - INFO - Took 11 steps
2019-04-11 23:36:30,910 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:30,915 - experiments.base - INFO - 7/10 Processing VI with discount factor 0.6
2019-04-11 23:36:30,951 - experiments.base - INFO - Took 16 steps
2019-04-11 23:36:32,931 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:32,938 - experiments.base - INFO - 8/10 Processing VI with discount factor 0.7
2019-04-11 23:36:32,990 - experiments.base - INFO - Took 23 steps
2019-04-11 23:36:35,029 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:35,036 - experiments.base - INFO - 9/10 Processing VI with discount factor 0.8
2019-04-11 23:36:35,108 - experiments.base - INFO - Took 33 steps
2019-04-11 23:36:36,121 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-11 23:36:36,158 - experiments.base - INFO - 10/10 Processing VI with discount factor 0.9
2019-04-11 23:36:36,233 - experiments.base - INFO - Took 33 steps
2019-04-11 23:36:37,194 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-11 23:36:37,230 - __main__ - INFO - Running VI experiment: Mazeworld (15x15)
2019-04-11 23:36:37,233 - experiments.base - INFO - Searching VI in 10 dimensions
2019-04-11 23:36:37,233 - experiments.base - INFO - 1/10 Processing VI with discount factor 0.0
2019-04-11 23:36:37,246 - experiments.base - INFO - Took 2 steps
2019-04-11 23:36:40,165 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:40,171 - experiments.base - INFO - 2/10 Processing VI with discount factor 0.1
2019-04-11 23:36:40,214 - experiments.base - INFO - Took 7 steps
2019-04-11 23:36:43,122 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:43,128 - experiments.base - INFO - 3/10 Processing VI with discount factor 0.2
2019-04-11 23:36:43,184 - experiments.base - INFO - Took 9 steps
2019-04-11 23:36:46,091 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:46,098 - experiments.base - INFO - 4/10 Processing VI with discount factor 0.3
2019-04-11 23:36:46,163 - experiments.base - INFO - Took 11 steps
2019-04-11 23:36:49,075 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:49,081 - experiments.base - INFO - 5/10 Processing VI with discount factor 0.4
2019-04-11 23:36:49,164 - experiments.base - INFO - Took 14 steps
2019-04-11 23:36:52,138 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:52,144 - experiments.base - INFO - 6/10 Processing VI with discount factor 0.5
2019-04-11 23:36:52,253 - experiments.base - INFO - Took 18 steps
2019-04-11 23:36:55,239 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:55,246 - experiments.base - INFO - 7/10 Processing VI with discount factor 0.6
2019-04-11 23:36:55,388 - experiments.base - INFO - Took 24 steps
2019-04-11 23:36:58,351 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:36:58,358 - experiments.base - INFO - 8/10 Processing VI with discount factor 0.7
2019-04-11 23:36:58,535 - experiments.base - INFO - Took 30 steps
2019-04-11 23:37:01,562 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:37:01,568 - experiments.base - INFO - 9/10 Processing VI with discount factor 0.8
2019-04-11 23:37:01,775 - experiments.base - INFO - Took 35 steps
2019-04-11 23:37:03,713 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-11 23:37:03,720 - experiments.base - INFO - 10/10 Processing VI with discount factor 0.9
2019-04-11 23:37:03,930 - experiments.base - INFO - Took 35 steps
2019-04-11 23:37:05,928 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-11 23:37:05,936 - __main__ - INFO - Running Q experiment: Mazeworld (4x4)
2019-04-11 23:37:05,937 - experiments.base - INFO - Searching Q in 180 dimensions
2019-04-11 23:37:05,937 - experiments.base - INFO - 1/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:37:09,180 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:10,757 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:37:10,765 - experiments.base - INFO - 2/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:37:11,372 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:11,805 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:11,815 - experiments.base - INFO - 3/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:37:12,403 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:12,845 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:12,855 - experiments.base - INFO - 4/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:37:13,486 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:13,951 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:13,963 - experiments.base - INFO - 5/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:37:14,625 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:15,101 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:15,114 - experiments.base - INFO - 6/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:37:15,778 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:16,308 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:16,319 - experiments.base - INFO - 7/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:37:16,940 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:17,378 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:17,388 - experiments.base - INFO - 8/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:37:17,989 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:18,430 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:18,444 - experiments.base - INFO - 9/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:37:19,058 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:19,490 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:19,502 - experiments.base - INFO - 10/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:37:20,098 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:20,571 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:20,582 - experiments.base - INFO - 11/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:37:23,845 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:25,466 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:37:25,473 - experiments.base - INFO - 12/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:37:26,125 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:26,556 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:26,566 - experiments.base - INFO - 13/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:37:27,213 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:27,641 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:27,653 - experiments.base - INFO - 14/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:37:28,315 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:28,753 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:28,763 - experiments.base - INFO - 15/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:37:29,430 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:29,862 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:29,874 - experiments.base - INFO - 16/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:37:30,552 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:30,990 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:31,002 - experiments.base - INFO - 17/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:37:31,664 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:32,134 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:32,144 - experiments.base - INFO - 18/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:37:32,801 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:33,250 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:33,260 - experiments.base - INFO - 19/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:37:33,901 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:34,336 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:34,348 - experiments.base - INFO - 20/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:37:35,117 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:35,552 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:35,565 - experiments.base - INFO - 21/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:37:38,819 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:40,391 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:37:40,400 - experiments.base - INFO - 22/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:37:41,125 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:41,594 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:41,605 - experiments.base - INFO - 23/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:37:42,319 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:42,750 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:42,762 - experiments.base - INFO - 24/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:37:43,490 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:43,930 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:43,943 - experiments.base - INFO - 25/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:37:44,687 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:45,119 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:45,131 - experiments.base - INFO - 26/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:37:45,855 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:46,286 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:46,296 - experiments.base - INFO - 27/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:37:47,023 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:47,457 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:47,470 - experiments.base - INFO - 28/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:37:48,211 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:48,686 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:48,697 - experiments.base - INFO - 29/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:37:49,487 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:49,941 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:49,953 - experiments.base - INFO - 30/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:37:50,736 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:37:51,168 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:37:51,180 - experiments.base - INFO - 31/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:37:57,732 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:37:59,267 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:37:59,276 - experiments.base - INFO - 32/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:38:05,812 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:38:07,331 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:38:07,338 - experiments.base - INFO - 33/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:38:14,611 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:38:16,196 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:38:16,203 - experiments.base - INFO - 34/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:38:22,867 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:38:24,474 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:38:24,482 - experiments.base - INFO - 35/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:38:31,049 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:38:32,578 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:38:32,585 - experiments.base - INFO - 36/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:38:39,122 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:38:40,657 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:38:40,665 - experiments.base - INFO - 37/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:38:47,174 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:38:48,690 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:38:48,698 - experiments.base - INFO - 38/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:38:55,256 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:38:56,799 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:38:56,808 - experiments.base - INFO - 39/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:39:03,407 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:39:04,990 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:39:04,999 - experiments.base - INFO - 40/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:39:11,538 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:39:13,071 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:39:13,078 - experiments.base - INFO - 41/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:39:19,632 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:39:21,203 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:39:21,211 - experiments.base - INFO - 42/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:39:27,776 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:39:29,301 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:39:29,308 - experiments.base - INFO - 43/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:39:35,844 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:39:37,374 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:39:37,381 - experiments.base - INFO - 44/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:39:38,283 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:39:38,750 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:39:38,762 - experiments.base - INFO - 45/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:39:39,484 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:39:39,915 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:39:39,927 - experiments.base - INFO - 46/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:39:40,651 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:39:41,095 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:39:41,107 - experiments.base - INFO - 47/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:39:47,602 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:39:49,131 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:39:49,138 - experiments.base - INFO - 48/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:39:55,651 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:39:57,163 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:39:57,171 - experiments.base - INFO - 49/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:39:58,033 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:39:58,476 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:39:58,486 - experiments.base - INFO - 50/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:40:05,002 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:40:06,572 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:40:06,581 - experiments.base - INFO - 51/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:40:38,789 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:40:40,355 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:40:40,365 - experiments.base - INFO - 52/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:40:41,148 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:40:41,572 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:40:41,585 - experiments.base - INFO - 53/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:40:42,328 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:40:42,763 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:40:42,775 - experiments.base - INFO - 54/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:40:43,561 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:40:44,006 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:40:44,019 - experiments.base - INFO - 55/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:40:44,779 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:40:45,247 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:40:45,260 - experiments.base - INFO - 56/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:40:46,039 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:40:46,463 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:40:46,474 - experiments.base - INFO - 57/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:40:47,338 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:40:47,763 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:40:47,776 - experiments.base - INFO - 58/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:40:48,607 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:40:49,032 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:40:49,045 - experiments.base - INFO - 59/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:40:49,809 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:40:50,246 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:40:50,259 - experiments.base - INFO - 60/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:40:56,803 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:40:58,323 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:40:58,332 - experiments.base - INFO - 61/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:41:16,918 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:41:18,525 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:41:18,532 - experiments.base - INFO - 62/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:41:19,102 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:41:19,535 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:41:19,546 - experiments.base - INFO - 63/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:41:20,141 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:41:20,578 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:41:20,589 - experiments.base - INFO - 64/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:41:21,177 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:41:21,611 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:41:21,622 - experiments.base - INFO - 65/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:41:22,226 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:41:22,651 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:41:22,664 - experiments.base - INFO - 66/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:41:23,259 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:41:23,740 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:41:23,752 - experiments.base - INFO - 67/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:41:24,351 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:41:24,786 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:41:24,799 - experiments.base - INFO - 68/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:41:25,372 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:41:25,808 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:41:25,822 - experiments.base - INFO - 69/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:41:26,397 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:41:26,867 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:41:26,878 - experiments.base - INFO - 70/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:41:27,497 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:41:27,936 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:41:27,947 - experiments.base - INFO - 71/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:41:46,058 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:41:47,628 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:41:47,635 - experiments.base - INFO - 72/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:41:48,278 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:41:48,742 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:41:48,753 - experiments.base - INFO - 73/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:41:49,388 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:41:49,832 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:41:49,844 - experiments.base - INFO - 74/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:41:50,503 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:41:50,944 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:41:50,957 - experiments.base - INFO - 75/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:41:51,584 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:41:52,032 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:41:52,045 - experiments.base - INFO - 76/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:41:52,694 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:41:53,122 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:41:53,134 - experiments.base - INFO - 77/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:41:53,795 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:41:54,266 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:41:54,279 - experiments.base - INFO - 78/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:41:54,930 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:41:55,365 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:41:55,377 - experiments.base - INFO - 79/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:41:56,016 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:41:56,447 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:41:56,459 - experiments.base - INFO - 80/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:41:57,121 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:41:57,566 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:41:57,578 - experiments.base - INFO - 81/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:42:15,565 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:42:17,138 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:42:17,157 - experiments.base - INFO - 82/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:42:17,880 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:42:18,302 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:42:18,315 - experiments.base - INFO - 83/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:42:19,036 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:42:19,493 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:42:19,506 - experiments.base - INFO - 84/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:42:20,233 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:42:20,663 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:42:20,674 - experiments.base - INFO - 85/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:42:21,401 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:42:21,848 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:42:21,861 - experiments.base - INFO - 86/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:42:22,585 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:42:23,019 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:42:23,032 - experiments.base - INFO - 87/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:42:23,733 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:42:24,164 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:42:24,176 - experiments.base - INFO - 88/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:42:24,903 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:42:25,371 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:42:25,384 - experiments.base - INFO - 89/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:42:26,118 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:42:26,562 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:42:26,575 - experiments.base - INFO - 90/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:42:27,309 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:42:27,763 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:42:27,775 - experiments.base - INFO - 91/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:42:34,296 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:42:35,848 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:42:35,857 - experiments.base - INFO - 92/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:42:42,365 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:42:43,904 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:42:43,911 - experiments.base - INFO - 93/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:42:50,438 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:42:51,980 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:42:51,990 - experiments.base - INFO - 94/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:42:58,476 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:43:00,032 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:43:00,040 - experiments.base - INFO - 95/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:43:06,581 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:43:08,105 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:43:08,114 - experiments.base - INFO - 96/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:43:14,641 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:43:16,160 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:43:16,167 - experiments.base - INFO - 97/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:43:22,704 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:43:24,223 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:43:24,233 - experiments.base - INFO - 98/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:43:30,744 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:43:32,267 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:43:32,276 - experiments.base - INFO - 99/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:43:38,796 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:43:40,348 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:43:40,357 - experiments.base - INFO - 100/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:43:46,858 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:43:48,375 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:43:48,384 - experiments.base - INFO - 101/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:43:54,931 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:43:56,448 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:43:56,457 - experiments.base - INFO - 102/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:43:57,164 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:43:57,598 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:43:57,611 - experiments.base - INFO - 103/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:44:04,104 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:44:05,645 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:44:05,654 - experiments.base - INFO - 104/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:44:12,137 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:44:13,667 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:44:13,677 - experiments.base - INFO - 105/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:44:46,144 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:44:47,730 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:44:47,740 - experiments.base - INFO - 106/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:44:54,266 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:44:55,790 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:44:55,799 - experiments.base - INFO - 107/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:45:02,371 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:45:03,887 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:45:03,894 - experiments.base - INFO - 108/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:45:36,309 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:45:37,846 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:45:37,855 - experiments.base - INFO - 109/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:45:38,803 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:45:39,233 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:45:39,246 - experiments.base - INFO - 110/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:45:45,736 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:45:47,299 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:45:47,306 - experiments.base - INFO - 111/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:46:19,437 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:46:21,013 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:46:21,023 - experiments.base - INFO - 112/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:46:21,789 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:46:22,217 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:46:22,230 - experiments.base - INFO - 113/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:46:28,831 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:46:30,358 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:46:30,371 - experiments.base - INFO - 114/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:46:31,165 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:46:31,596 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:46:31,608 - experiments.base - INFO - 115/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:46:32,480 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:46:32,898 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:46:32,911 - experiments.base - INFO - 116/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:46:39,461 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:46:41,036 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:46:41,043 - experiments.base - INFO - 117/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:46:41,799 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:46:42,216 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:46:42,229 - experiments.base - INFO - 118/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:46:43,030 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:46:43,456 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:46:43,467 - experiments.base - INFO - 119/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:46:50,056 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:46:51,592 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:46:51,601 - experiments.base - INFO - 120/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:46:52,391 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:46:52,818 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:46:52,829 - experiments.base - INFO - 121/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:47:20,186 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:47:21,813 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:47:21,822 - experiments.base - INFO - 122/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:47:22,398 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:47:22,818 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:47:22,828 - experiments.base - INFO - 123/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:47:23,398 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:47:23,838 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:47:23,851 - experiments.base - INFO - 124/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:47:24,424 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:47:24,861 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:47:24,872 - experiments.base - INFO - 125/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:47:25,443 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:47:25,858 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:47:25,872 - experiments.base - INFO - 126/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:47:26,461 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:47:26,898 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:47:26,911 - experiments.base - INFO - 127/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:47:27,494 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:47:27,966 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:47:27,980 - experiments.base - INFO - 128/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:47:28,566 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:47:28,996 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:47:29,009 - experiments.base - INFO - 129/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:47:29,598 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:47:30,032 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:47:30,046 - experiments.base - INFO - 130/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:47:30,635 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:47:31,065 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:47:31,078 - experiments.base - INFO - 131/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:47:57,880 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:47:59,430 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:47:59,440 - experiments.base - INFO - 132/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:48:00,076 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:48:00,540 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:48:00,552 - experiments.base - INFO - 133/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:48:01,190 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:48:01,605 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:48:01,618 - experiments.base - INFO - 134/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:48:02,260 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:48:02,680 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:48:02,693 - experiments.base - INFO - 135/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:48:03,326 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:48:03,752 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:48:03,765 - experiments.base - INFO - 136/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:48:04,414 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:48:04,861 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:48:04,874 - experiments.base - INFO - 137/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:48:05,517 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:48:05,944 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:48:05,957 - experiments.base - INFO - 138/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:48:06,609 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:48:07,065 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:48:07,085 - experiments.base - INFO - 139/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:48:07,737 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:48:08,163 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:48:08,184 - experiments.base - INFO - 140/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:48:08,844 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:48:09,275 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:48:09,296 - experiments.base - INFO - 141/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:48:35,648 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:48:37,207 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:48:37,224 - experiments.base - INFO - 142/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:48:37,930 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:48:38,354 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:48:38,375 - experiments.base - INFO - 143/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:48:39,084 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:48:39,542 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:48:39,555 - experiments.base - INFO - 144/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:48:40,278 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:48:40,693 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:48:40,706 - experiments.base - INFO - 145/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:48:41,424 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:48:41,844 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:48:41,857 - experiments.base - INFO - 146/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:48:42,568 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:48:42,990 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:48:43,003 - experiments.base - INFO - 147/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:48:43,730 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:48:44,154 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:48:44,165 - experiments.base - INFO - 148/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:48:44,872 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:48:45,295 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:48:45,309 - experiments.base - INFO - 149/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:48:46,028 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:48:46,492 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:48:46,506 - experiments.base - INFO - 150/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:48:47,224 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:48:47,664 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:48:47,677 - experiments.base - INFO - 151/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:48:54,236 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:48:55,736 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:48:55,746 - experiments.base - INFO - 152/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:49:02,276 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:49:03,786 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:49:03,795 - experiments.base - INFO - 153/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:49:10,351 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:49:11,898 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:49:11,907 - experiments.base - INFO - 154/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:49:18,426 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:49:19,984 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:49:19,993 - experiments.base - INFO - 155/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:49:26,496 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:49:28,010 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:49:28,019 - experiments.base - INFO - 156/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:49:34,500 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:49:36,065 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:49:36,075 - experiments.base - INFO - 157/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:49:42,582 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:49:44,102 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:49:44,111 - experiments.base - INFO - 158/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:49:50,655 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:49:52,200 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:49:52,209 - experiments.base - INFO - 159/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:49:58,720 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:50:00,246 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:50:00,255 - experiments.base - INFO - 160/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:50:06,811 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:50:08,371 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:50:08,380 - experiments.base - INFO - 161/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:50:14,875 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:50:16,411 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:50:16,420 - experiments.base - INFO - 162/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:50:22,967 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:50:24,490 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:50:24,500 - experiments.base - INFO - 163/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:50:31,016 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:50:32,542 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:50:32,552 - experiments.base - INFO - 164/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:50:39,085 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:50:40,627 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:50:40,635 - experiments.base - INFO - 165/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:50:47,142 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:50:48,687 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:50:48,696 - experiments.base - INFO - 166/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:50:55,226 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:50:56,769 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:50:56,779 - experiments.base - INFO - 167/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:51:03,323 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:51:04,874 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:51:04,884 - experiments.base - INFO - 168/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:51:11,388 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:51:12,921 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:51:12,931 - experiments.base - INFO - 169/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:51:13,825 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:51:14,260 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:51:14,273 - experiments.base - INFO - 170/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:51:20,842 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:51:22,397 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:51:22,405 - experiments.base - INFO - 171/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:51:54,845 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:51:56,424 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:51:56,433 - experiments.base - INFO - 172/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:52:02,941 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:52:04,453 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:52:04,461 - experiments.base - INFO - 173/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:52:05,819 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:06,246 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:52:06,260 - experiments.base - INFO - 174/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:52:06,996 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:07,433 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:52:07,446 - experiments.base - INFO - 175/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:52:08,782 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:09,206 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:52:09,220 - experiments.base - INFO - 176/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:52:09,982 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:10,453 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:52:10,466 - experiments.base - INFO - 177/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:52:11,319 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:11,743 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:52:11,756 - experiments.base - INFO - 178/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:52:12,618 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:13,043 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:52:13,058 - experiments.base - INFO - 179/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:52:13,851 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:14,267 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:52:14,280 - experiments.base - INFO - 180/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:52:15,040 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:15,486 - experiments.base - INFO - reward_mean: 0.1666666666666667, reward_median: 0.16666666666666666, reward_std: 5.551115123125783e-17, reward_max: 0.16666666666666666, reward_min: 0.16666666666666666, runs: 100
2019-04-11 23:52:15,500 - __main__ - INFO - Running Q experiment: Mazeworld (5x5)
2019-04-11 23:52:15,505 - experiments.base - INFO - Searching Q in 180 dimensions
2019-04-11 23:52:15,505 - experiments.base - INFO - 1/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:52:27,763 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:29,387 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:52:29,394 - experiments.base - INFO - 2/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:52:30,378 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:30,892 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:52:30,903 - experiments.base - INFO - 3/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:52:31,790 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:32,273 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:52:32,282 - experiments.base - INFO - 4/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:52:33,187 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:33,671 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:52:33,680 - experiments.base - INFO - 5/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:52:34,563 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:35,079 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:52:35,085 - experiments.base - INFO - 6/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:52:35,927 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:36,411 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:52:36,418 - experiments.base - INFO - 7/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:52:37,276 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:37,796 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:52:37,803 - experiments.base - INFO - 8/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:52:38,706 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:39,187 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:52:39,194 - experiments.base - INFO - 9/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:52:40,111 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:40,599 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:52:40,607 - experiments.base - INFO - 10/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:52:41,753 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:42,244 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:52:42,252 - experiments.base - INFO - 11/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:52:54,454 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:56,082 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:52:56,089 - experiments.base - INFO - 12/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:52:57,096 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:57,585 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:52:57,592 - experiments.base - INFO - 13/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:52:58,532 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:52:59,095 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:52:59,105 - experiments.base - INFO - 14/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:53:00,036 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:53:00,526 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:53:00,532 - experiments.base - INFO - 15/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:53:01,463 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:53:01,951 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:53:01,959 - experiments.base - INFO - 16/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:53:02,887 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:53:03,375 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:53:03,382 - experiments.base - INFO - 17/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:53:04,279 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:53:04,778 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:53:04,785 - experiments.base - INFO - 18/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:53:05,747 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:53:06,275 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:53:06,282 - experiments.base - INFO - 19/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:53:07,167 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:53:07,651 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:53:07,658 - experiments.base - INFO - 20/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:53:08,920 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:53:09,414 - experiments.base - INFO - reward_mean: 0.08333333333333336, reward_median: 0.08333333333333333, reward_std: 2.7755575615628914e-17, reward_max: 0.08333333333333333, reward_min: 0.08333333333333333, runs: 100
2019-04-11 23:53:09,427 - experiments.base - INFO - 21/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:53:21,526 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:53:23,155 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:53:23,164 - experiments.base - INFO - 22/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:53:24,250 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:53:24,732 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:53:24,740 - experiments.base - INFO - 23/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:53:25,759 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:53:26,242 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:53:26,250 - experiments.base - INFO - 24/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:53:27,267 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:53:27,805 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:53:27,812 - experiments.base - INFO - 25/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:53:28,795 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:53:29,276 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:53:29,285 - experiments.base - INFO - 26/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:53:30,321 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:53:30,849 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:53:30,857 - experiments.base - INFO - 27/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:53:31,996 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:53:32,490 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:53:32,499 - experiments.base - INFO - 28/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:53:33,539 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:53:34,043 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:53:34,049 - experiments.base - INFO - 29/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:53:35,061 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:53:35,611 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:53:35,621 - experiments.base - INFO - 30/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:53:36,638 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:53:37,132 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:53:37,141 - experiments.base - INFO - 31/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:53:43,707 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:53:45,305 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:53:45,313 - experiments.base - INFO - 32/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:53:51,917 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:53:53,483 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:53:53,493 - experiments.base - INFO - 33/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:54:00,073 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:54:01,648 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:54:01,655 - experiments.base - INFO - 34/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:54:08,220 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:54:09,799 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:54:09,808 - experiments.base - INFO - 35/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:54:16,372 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:54:17,976 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:54:17,984 - experiments.base - INFO - 36/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:54:24,548 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:54:26,121 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:54:26,131 - experiments.base - INFO - 37/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:54:32,653 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:54:34,239 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:54:34,247 - experiments.base - INFO - 38/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:54:40,802 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:54:42,395 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:54:42,405 - experiments.base - INFO - 39/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:54:48,941 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:54:50,549 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:54:50,558 - experiments.base - INFO - 40/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:54:57,092 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:54:58,707 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:54:58,716 - experiments.base - INFO - 41/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:55:05,276 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:55:06,846 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:55:06,855 - experiments.base - INFO - 42/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:55:13,368 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:55:14,934 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:55:14,943 - experiments.base - INFO - 43/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:55:21,476 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:55:23,059 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:55:23,066 - experiments.base - INFO - 44/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:55:29,578 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:55:31,134 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:55:31,141 - experiments.base - INFO - 45/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:55:37,687 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:55:39,253 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:55:39,262 - experiments.base - INFO - 46/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:55:45,796 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:55:47,387 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:55:47,395 - experiments.base - INFO - 47/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:55:53,944 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:55:55,526 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:55:55,535 - experiments.base - INFO - 48/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:56:02,026 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:56:03,582 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:56:03,591 - experiments.base - INFO - 49/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:56:10,111 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:56:11,668 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:56:11,677 - experiments.base - INFO - 50/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:56:18,266 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:56:19,861 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:56:19,869 - experiments.base - INFO - 51/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:56:26,421 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:56:28,022 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:56:28,030 - experiments.base - INFO - 52/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:56:34,565 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:56:36,153 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:56:36,160 - experiments.base - INFO - 53/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:56:42,648 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:56:44,214 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:56:44,223 - experiments.base - INFO - 54/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:56:50,823 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:56:52,401 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:56:52,410 - experiments.base - INFO - 55/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:56:58,963 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:57:00,551 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:57:00,559 - experiments.base - INFO - 56/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:57:07,096 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:57:08,676 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:57:08,684 - experiments.base - INFO - 57/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:57:15,471 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:57:17,190 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:57:17,197 - experiments.base - INFO - 58/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:57:23,910 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:57:25,473 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:57:25,483 - experiments.base - INFO - 59/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:57:32,035 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:57:33,615 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:57:33,624 - experiments.base - INFO - 60/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:57:40,190 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:57:41,759 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:57:41,767 - experiments.base - INFO - 61/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:58:09,844 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:58:11,460 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:58:11,467 - experiments.base - INFO - 62/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:58:12,286 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:58:12,815 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:58:12,822 - experiments.base - INFO - 63/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:58:13,621 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:58:14,107 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:58:14,115 - experiments.base - INFO - 64/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:58:14,908 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:58:15,394 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:58:15,403 - experiments.base - INFO - 65/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:58:16,194 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:58:16,688 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:58:16,697 - experiments.base - INFO - 66/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:58:17,493 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:58:17,983 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:58:17,992 - experiments.base - INFO - 67/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:58:18,801 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:58:19,289 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:58:19,298 - experiments.base - INFO - 68/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:58:20,131 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:58:20,667 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:58:20,674 - experiments.base - INFO - 69/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:58:21,516 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:58:22,019 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:58:22,028 - experiments.base - INFO - 70/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:58:22,871 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:58:23,355 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:58:23,365 - experiments.base - INFO - 71/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:58:51,569 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:58:53,181 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:58:53,191 - experiments.base - INFO - 72/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:58:54,072 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:58:54,556 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:58:54,566 - experiments.base - INFO - 73/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:58:55,434 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:58:55,966 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:58:55,973 - experiments.base - INFO - 74/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:58:56,871 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:58:57,357 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:58:57,365 - experiments.base - INFO - 75/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:58:58,247 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:58:58,743 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:58:58,750 - experiments.base - INFO - 76/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:58:59,630 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:59:00,125 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:59:00,134 - experiments.base - INFO - 77/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:59:01,020 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:59:01,513 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:59:01,523 - experiments.base - INFO - 78/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:59:02,398 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:59:02,885 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:59:02,894 - experiments.base - INFO - 79/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:59:04,045 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:59:04,578 - experiments.base - INFO - reward_mean: 0.08333333333333336, reward_median: 0.08333333333333333, reward_std: 2.7755575615628914e-17, reward_max: 0.08333333333333333, reward_min: 0.08333333333333333, runs: 100
2019-04-11 23:59:04,592 - experiments.base - INFO - 80/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:59:05,503 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:59:06,009 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:59:06,016 - experiments.base - INFO - 81/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:59:34,299 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:59:35,918 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:59:35,928 - experiments.base - INFO - 82/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-11 23:59:36,927 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:59:37,424 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:59:37,431 - experiments.base - INFO - 83/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-11 23:59:38,414 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:59:38,901 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:59:38,908 - experiments.base - INFO - 84/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-11 23:59:39,894 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:59:40,420 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:59:40,430 - experiments.base - INFO - 85/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-11 23:59:41,407 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:59:41,882 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:59:41,891 - experiments.base - INFO - 86/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-11 23:59:42,885 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:59:43,375 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:59:43,381 - experiments.base - INFO - 87/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-11 23:59:44,365 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:59:44,839 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:59:44,848 - experiments.base - INFO - 88/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-11 23:59:45,818 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:59:46,305 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:59:46,312 - experiments.base - INFO - 89/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-11 23:59:47,278 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:59:47,763 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:59:47,772 - experiments.base - INFO - 90/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-11 23:59:48,753 - experiments.base - INFO - Took 1000 episodes
2019-04-11 23:59:49,269 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-11 23:59:49,278 - experiments.base - INFO - 91/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-11 23:59:55,828 - experiments.base - INFO - Took 200 episodes
2019-04-11 23:59:57,403 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-11 23:59:57,411 - experiments.base - INFO - 92/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:00:03,990 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:00:05,572 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:00:05,579 - experiments.base - INFO - 93/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:00:12,130 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:00:13,724 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:00:13,733 - experiments.base - INFO - 94/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:00:20,390 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:00:21,980 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:00:21,986 - experiments.base - INFO - 95/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:00:28,548 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:00:30,151 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:00:30,160 - experiments.base - INFO - 96/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:00:36,743 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:00:38,302 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:00:38,311 - experiments.base - INFO - 97/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:00:44,835 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:00:46,401 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:00:46,410 - experiments.base - INFO - 98/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:00:52,947 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:00:54,573 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:00:54,582 - experiments.base - INFO - 99/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:01:01,108 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:01:02,703 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:01:02,710 - experiments.base - INFO - 100/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:01:09,262 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:01:10,832 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:01:10,841 - experiments.base - INFO - 101/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:01:17,417 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:01:19,036 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:01:19,045 - experiments.base - INFO - 102/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:01:25,589 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:01:27,230 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:01:27,239 - experiments.base - INFO - 103/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:01:33,828 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:01:35,438 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:01:35,446 - experiments.base - INFO - 104/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:01:41,982 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:01:43,563 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:01:43,572 - experiments.base - INFO - 105/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:01:50,142 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:01:51,733 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:01:51,742 - experiments.base - INFO - 106/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:01:58,315 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:01:59,930 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:01:59,937 - experiments.base - INFO - 107/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:02:06,476 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:02:08,033 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:02:08,042 - experiments.base - INFO - 108/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:02:14,569 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:02:16,135 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:02:16,144 - experiments.base - INFO - 109/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:02:22,730 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:02:24,306 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:02:24,315 - experiments.base - INFO - 110/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:02:30,849 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:02:32,417 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:02:32,426 - experiments.base - INFO - 111/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:02:38,970 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:02:40,549 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:02:40,558 - experiments.base - INFO - 112/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:02:47,065 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:02:48,677 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:02:48,684 - experiments.base - INFO - 113/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:02:55,253 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:02:56,819 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:02:56,828 - experiments.base - INFO - 114/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:03:03,365 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:03:04,973 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:03:04,984 - experiments.base - INFO - 115/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:03:11,532 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:03:13,111 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:03:13,118 - experiments.base - INFO - 116/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:03:19,687 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:03:21,278 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:03:21,288 - experiments.base - INFO - 117/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:03:27,841 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:03:29,440 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:03:29,450 - experiments.base - INFO - 118/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:03:35,976 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:03:37,548 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:03:37,559 - experiments.base - INFO - 119/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:03:44,076 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:03:45,650 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:03:45,657 - experiments.base - INFO - 120/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:03:52,227 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:03:53,786 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:03:53,792 - experiments.base - INFO - 121/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:04:24,795 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:04:26,391 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:04:26,400 - experiments.base - INFO - 122/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:04:27,188 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:04:27,665 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:04:27,674 - experiments.base - INFO - 123/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:04:28,454 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:04:28,990 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:04:28,996 - experiments.base - INFO - 124/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:04:29,788 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:04:30,269 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:04:30,276 - experiments.base - INFO - 125/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:04:31,061 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:04:31,538 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:04:31,548 - experiments.base - INFO - 126/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:04:32,346 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:04:32,823 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:04:32,835 - experiments.base - INFO - 127/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:04:33,628 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:04:34,102 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:04:34,111 - experiments.base - INFO - 128/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:04:34,937 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:04:35,467 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:04:35,477 - experiments.base - INFO - 129/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:04:36,358 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:04:36,825 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:04:36,832 - experiments.base - INFO - 130/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:04:37,618 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:04:38,098 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:04:38,107 - experiments.base - INFO - 131/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:05:08,971 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:05:10,575 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:05:10,584 - experiments.base - INFO - 132/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:05:11,456 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:05:11,926 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:05:11,934 - experiments.base - INFO - 133/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:05:12,826 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:05:13,305 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:05:13,312 - experiments.base - INFO - 134/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:05:14,183 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:05:14,706 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:05:14,713 - experiments.base - INFO - 135/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:05:15,582 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:05:16,059 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:05:16,068 - experiments.base - INFO - 136/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:05:16,948 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:05:17,420 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:05:17,427 - experiments.base - INFO - 137/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:05:18,302 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:05:18,779 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:05:18,788 - experiments.base - INFO - 138/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:05:19,664 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:05:20,160 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:05:20,167 - experiments.base - INFO - 139/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:05:21,042 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:05:21,565 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:05:21,573 - experiments.base - INFO - 140/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:05:22,470 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:05:22,947 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:05:22,956 - experiments.base - INFO - 141/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:05:54,017 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:05:55,607 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:05:55,614 - experiments.base - INFO - 142/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:05:56,605 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:05:57,086 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:05:57,095 - experiments.base - INFO - 143/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:05:58,096 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:05:58,585 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:05:58,592 - experiments.base - INFO - 144/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:05:59,609 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:06:00,082 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:06:00,091 - experiments.base - INFO - 145/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:06:01,075 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:06:01,602 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:06:01,611 - experiments.base - INFO - 146/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:06:02,605 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:06:03,092 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:06:03,099 - experiments.base - INFO - 147/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:06:04,085 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:06:04,584 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:06:04,591 - experiments.base - INFO - 148/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:06:05,622 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:06:06,099 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:06:06,108 - experiments.base - INFO - 149/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:06:07,105 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:06:07,592 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:06:07,599 - experiments.base - INFO - 150/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:06:08,621 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:06:09,147 - experiments.base - INFO - reward_mean: 0.125, reward_median: 0.125, reward_std: 0.0, reward_max: 0.125, reward_min: 0.125, runs: 100
2019-04-12 00:06:09,154 - experiments.base - INFO - 151/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:06:15,668 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:06:17,243 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:06:17,252 - experiments.base - INFO - 152/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:06:23,772 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:06:25,335 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:06:25,342 - experiments.base - INFO - 153/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:06:32,000 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:06:33,565 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:06:33,572 - experiments.base - INFO - 154/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:06:40,148 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:06:41,726 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:06:41,733 - experiments.base - INFO - 155/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:06:48,269 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:06:49,855 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:06:49,864 - experiments.base - INFO - 156/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:06:56,431 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:06:58,030 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:06:58,039 - experiments.base - INFO - 157/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:07:04,558 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:07:06,150 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:07:06,161 - experiments.base - INFO - 158/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:07:12,696 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:07:14,275 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:07:14,282 - experiments.base - INFO - 159/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:07:20,812 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:07:22,411 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:07:22,418 - experiments.base - INFO - 160/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:07:28,930 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:07:30,490 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:07:30,497 - experiments.base - INFO - 161/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:07:37,066 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:07:38,671 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:07:38,678 - experiments.base - INFO - 162/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:07:45,177 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:07:46,740 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:07:46,747 - experiments.base - INFO - 163/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:07:53,276 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:07:54,838 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:07:54,848 - experiments.base - INFO - 164/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:08:01,351 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:08:02,917 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:08:02,926 - experiments.base - INFO - 165/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:08:09,473 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:08:11,066 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:08:11,076 - experiments.base - INFO - 166/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:08:17,686 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:08:19,250 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:08:19,257 - experiments.base - INFO - 167/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:08:25,862 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:08:27,456 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:08:27,464 - experiments.base - INFO - 168/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:08:33,992 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:08:35,589 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:08:35,598 - experiments.base - INFO - 169/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:08:42,104 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:08:43,670 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:08:43,677 - experiments.base - INFO - 170/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:08:50,220 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:08:51,812 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:08:51,819 - experiments.base - INFO - 171/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:08:58,395 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:09:00,005 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:09:00,012 - experiments.base - INFO - 172/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:09:06,558 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:09:08,158 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:09:08,167 - experiments.base - INFO - 173/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:09:14,684 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:09:16,253 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:09:16,259 - experiments.base - INFO - 174/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:09:22,819 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:09:24,380 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:09:24,388 - experiments.base - INFO - 175/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:09:30,926 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:09:32,480 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:09:32,487 - experiments.base - INFO - 176/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:09:39,066 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:09:40,625 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:09:40,638 - experiments.base - INFO - 177/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:09:47,186 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:09:48,753 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:09:48,760 - experiments.base - INFO - 178/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:09:55,311 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:09:56,911 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:09:56,921 - experiments.base - INFO - 179/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:10:03,428 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:10:05,015 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:10:05,026 - experiments.base - INFO - 180/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:10:37,367 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:10:38,948 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:10:38,959 - __main__ - INFO - Running Q experiment: Mazeworld (11x11)
2019-04-12 00:10:38,963 - experiments.base - INFO - Searching Q in 180 dimensions
2019-04-12 00:10:38,963 - experiments.base - INFO - 1/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:11:10,743 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:11:13,016 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:11:13,025 - experiments.base - INFO - 2/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:11:20,658 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:11:21,862 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:11:21,875 - experiments.base - INFO - 3/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:11:28,219 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:11:29,355 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:11:29,368 - experiments.base - INFO - 4/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:11:35,362 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:11:36,538 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:11:36,551 - experiments.base - INFO - 5/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:11:41,780 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:11:42,983 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:11:42,996 - experiments.base - INFO - 6/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:11:48,213 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:11:49,394 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:11:49,407 - experiments.base - INFO - 7/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:11:54,171 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:11:55,368 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:11:55,380 - experiments.base - INFO - 8/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:12:00,183 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:12:01,332 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:12:01,344 - experiments.base - INFO - 9/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:12:06,440 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:12:07,582 - experiments.base - INFO - reward_mean: 0.03125, reward_median: 0.03125, reward_std: 0.0, reward_max: 0.03125, reward_min: 0.03125, runs: 100
2019-04-12 00:12:07,595 - experiments.base - INFO - 10/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:12:15,052 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:12:16,250 - experiments.base - INFO - reward_mean: 0.03125, reward_median: 0.03125, reward_std: 0.0, reward_max: 0.03125, reward_min: 0.03125, runs: 100
2019-04-12 00:12:16,257 - experiments.base - INFO - 11/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:12:48,470 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:12:50,749 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:12:50,756 - experiments.base - INFO - 12/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:12:58,296 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:12:59,437 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:12:59,450 - experiments.base - INFO - 13/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:13:05,720 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:13:06,914 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:13:06,927 - experiments.base - INFO - 14/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:13:12,686 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:13:13,845 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:13:13,858 - experiments.base - INFO - 15/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:13:19,131 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:13:20,322 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:13:20,334 - experiments.base - INFO - 16/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:13:25,292 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:13:26,497 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:13:26,510 - experiments.base - INFO - 17/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:13:31,344 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:13:32,513 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:13:32,525 - experiments.base - INFO - 18/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:13:37,250 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:13:38,446 - experiments.base - INFO - reward_mean: 0.03125, reward_median: 0.03125, reward_std: 0.0, reward_max: 0.03125, reward_min: 0.03125, runs: 100
2019-04-12 00:13:38,453 - experiments.base - INFO - 19/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:13:43,506 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:13:44,661 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:13:44,674 - experiments.base - INFO - 20/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:13:50,436 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:13:51,582 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:13:51,596 - experiments.base - INFO - 21/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:14:23,780 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:14:26,081 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:14:26,088 - experiments.base - INFO - 22/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:14:33,713 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:14:34,867 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:14:34,880 - experiments.base - INFO - 23/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:14:41,411 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:14:42,542 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:14:42,555 - experiments.base - INFO - 24/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:14:48,190 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:14:49,385 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:14:49,398 - experiments.base - INFO - 25/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:14:54,691 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:14:55,834 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:14:55,846 - experiments.base - INFO - 26/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:15:00,822 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:15:01,976 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:15:01,990 - experiments.base - INFO - 27/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:15:07,134 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:15:08,335 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:15:08,349 - experiments.base - INFO - 28/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:15:13,036 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:15:14,167 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:15:14,180 - experiments.base - INFO - 29/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:15:18,874 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:15:20,086 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:15:20,098 - experiments.base - INFO - 30/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:15:24,822 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:15:25,993 - experiments.base - INFO - reward_mean: 0.03125, reward_median: 0.03125, reward_std: 0.0, reward_max: 0.03125, reward_min: 0.03125, runs: 100
2019-04-12 00:15:26,003 - experiments.base - INFO - 31/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:15:32,677 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:15:34,900 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:15:34,907 - experiments.base - INFO - 32/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:15:41,621 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:15:43,871 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:15:43,881 - experiments.base - INFO - 33/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:15:50,572 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:15:52,766 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:15:52,778 - experiments.base - INFO - 34/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:15:59,444 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:16:01,621 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:16:01,631 - experiments.base - INFO - 35/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:16:08,336 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:16:10,575 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:16:10,585 - experiments.base - INFO - 36/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:16:17,293 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:16:19,519 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:16:19,528 - experiments.base - INFO - 37/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:16:26,286 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:16:28,480 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:16:28,490 - experiments.base - INFO - 38/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:16:35,161 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:16:37,403 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:16:37,414 - experiments.base - INFO - 39/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:16:44,048 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:16:46,243 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:16:46,253 - experiments.base - INFO - 40/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:16:52,923 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:16:55,183 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:16:55,193 - experiments.base - INFO - 41/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:17:01,828 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:17:04,028 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:17:04,038 - experiments.base - INFO - 42/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:17:10,730 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:17:12,926 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:17:12,936 - experiments.base - INFO - 43/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:17:20,186 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:17:22,434 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:17:22,446 - experiments.base - INFO - 44/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:17:29,109 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:17:31,319 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:17:31,328 - experiments.base - INFO - 45/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:17:38,033 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:17:40,230 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:17:40,239 - experiments.base - INFO - 46/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:17:46,887 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:17:49,131 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:17:49,140 - experiments.base - INFO - 47/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:17:55,798 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:17:58,005 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:17:58,015 - experiments.base - INFO - 48/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:18:04,720 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:18:06,936 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:18:06,948 - experiments.base - INFO - 49/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:18:13,628 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:18:15,875 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:18:15,882 - experiments.base - INFO - 50/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:18:22,575 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:18:24,819 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:18:24,829 - experiments.base - INFO - 51/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:18:31,500 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:18:33,742 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:18:33,753 - experiments.base - INFO - 52/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:18:40,441 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:18:42,637 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:18:42,645 - experiments.base - INFO - 53/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:18:49,306 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:18:51,536 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:18:51,545 - experiments.base - INFO - 54/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:18:58,237 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:19:00,480 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:19:00,487 - experiments.base - INFO - 55/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:19:07,186 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:19:09,377 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:19:09,385 - experiments.base - INFO - 56/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:19:16,068 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:19:18,259 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:19:18,269 - experiments.base - INFO - 57/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:19:24,934 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:19:27,177 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:19:27,186 - experiments.base - INFO - 58/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:19:33,796 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:19:36,033 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:19:36,042 - experiments.base - INFO - 59/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:19:42,700 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:19:44,903 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:19:44,911 - experiments.base - INFO - 60/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:19:51,615 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:19:53,878 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:19:53,885 - experiments.base - INFO - 61/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:20:27,161 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:20:29,414 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:20:29,421 - experiments.base - INFO - 62/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:20:33,056 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:20:34,232 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:20:34,244 - experiments.base - INFO - 63/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:20:37,795 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:20:38,937 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:20:38,950 - experiments.base - INFO - 64/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:20:42,364 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:20:43,493 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:20:43,506 - experiments.base - INFO - 65/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:20:46,901 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:20:48,082 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:20:48,096 - experiments.base - INFO - 66/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:20:51,506 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:20:52,638 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:20:52,651 - experiments.base - INFO - 67/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:20:55,940 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:20:57,084 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:20:57,098 - experiments.base - INFO - 68/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:21:00,676 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:21:01,865 - experiments.base - INFO - reward_mean: 0.03125, reward_median: 0.03125, reward_std: 0.0, reward_max: 0.03125, reward_min: 0.03125, runs: 100
2019-04-12 00:21:01,878 - experiments.base - INFO - 69/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:21:05,507 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:21:06,645 - experiments.base - INFO - reward_mean: 0.03125, reward_median: 0.03125, reward_std: 0.0, reward_max: 0.03125, reward_min: 0.03125, runs: 100
2019-04-12 00:21:06,654 - experiments.base - INFO - 70/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:21:10,188 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:21:11,335 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:21:11,349 - experiments.base - INFO - 71/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:21:44,269 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:21:46,545 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:21:46,553 - experiments.base - INFO - 72/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:21:50,285 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:21:51,424 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:21:51,437 - experiments.base - INFO - 73/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:21:54,943 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:21:56,121 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:21:56,134 - experiments.base - INFO - 74/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:21:59,582 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:22:00,717 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:22:00,730 - experiments.base - INFO - 75/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:22:04,095 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:22:05,259 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:22:05,272 - experiments.base - INFO - 76/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:22:08,664 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:22:09,854 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:22:09,868 - experiments.base - INFO - 77/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:22:13,213 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:22:14,453 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:22:14,466 - experiments.base - INFO - 78/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:22:17,891 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:22:19,045 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:22:19,059 - experiments.base - INFO - 79/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:22:22,536 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:22:23,720 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:22:23,734 - experiments.base - INFO - 80/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:22:27,269 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:22:28,405 - experiments.base - INFO - reward_mean: 0.03125, reward_median: 0.03125, reward_std: 0.0, reward_max: 0.03125, reward_min: 0.03125, runs: 100
2019-04-12 00:22:28,415 - experiments.base - INFO - 81/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:23:01,440 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:23:03,664 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:23:03,674 - experiments.base - INFO - 82/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:23:07,441 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:23:08,628 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:23:08,642 - experiments.base - INFO - 83/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:23:12,214 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:23:13,359 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:23:13,372 - experiments.base - INFO - 84/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:23:16,941 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:23:18,128 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:23:18,142 - experiments.base - INFO - 85/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:23:21,825 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:23:22,957 - experiments.base - INFO - reward_mean: 0.03125, reward_median: 0.03125, reward_std: 0.0, reward_max: 0.03125, reward_min: 0.03125, runs: 100
2019-04-12 00:23:22,967 - experiments.base - INFO - 86/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:23:26,392 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:23:27,517 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:23:27,532 - experiments.base - INFO - 87/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:23:31,032 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:23:32,219 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:23:32,232 - experiments.base - INFO - 88/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:23:35,681 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:23:36,829 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:23:36,842 - experiments.base - INFO - 89/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:23:40,434 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:23:41,618 - experiments.base - INFO - reward_mean: 0.03125, reward_median: 0.03125, reward_std: 0.0, reward_max: 0.03125, reward_min: 0.03125, runs: 100
2019-04-12 00:23:41,627 - experiments.base - INFO - 90/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:23:45,075 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:23:46,302 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:23:46,313 - experiments.base - INFO - 91/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:23:53,007 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:23:55,219 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:23:55,227 - experiments.base - INFO - 92/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:24:01,927 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:24:04,117 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:24:04,125 - experiments.base - INFO - 93/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:24:10,813 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:24:13,076 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:24:13,085 - experiments.base - INFO - 94/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:24:19,846 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:24:22,069 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:24:22,076 - experiments.base - INFO - 95/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:24:28,743 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:24:30,983 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:24:30,993 - experiments.base - INFO - 96/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:24:37,703 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:24:39,892 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:24:39,901 - experiments.base - INFO - 97/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:24:46,548 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:24:48,757 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:24:48,766 - experiments.base - INFO - 98/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:24:55,447 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:24:57,674 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:24:57,684 - experiments.base - INFO - 99/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:25:04,309 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:25:06,525 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:25:06,533 - experiments.base - INFO - 100/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:25:13,158 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:25:15,361 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:25:15,371 - experiments.base - INFO - 101/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:25:22,072 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:25:24,321 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:25:24,331 - experiments.base - INFO - 102/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:25:31,002 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:25:33,188 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:25:33,197 - experiments.base - INFO - 103/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:25:39,878 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:25:42,058 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:25:42,066 - experiments.base - INFO - 104/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:25:48,710 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:25:51,006 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:25:51,015 - experiments.base - INFO - 105/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:25:57,665 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:25:59,864 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:25:59,872 - experiments.base - INFO - 106/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:26:06,565 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:26:08,811 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:26:08,819 - experiments.base - INFO - 107/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:26:15,493 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:26:17,703 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:26:17,711 - experiments.base - INFO - 108/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:26:24,400 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:26:26,589 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:26:26,598 - experiments.base - INFO - 109/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:26:33,349 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:26:35,622 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:26:35,631 - experiments.base - INFO - 110/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:26:42,290 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:26:44,497 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:26:44,506 - experiments.base - INFO - 111/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:26:51,201 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:26:53,381 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:26:53,388 - experiments.base - INFO - 112/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:27:00,055 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:27:02,318 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:27:02,326 - experiments.base - INFO - 113/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:27:09,029 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:27:11,236 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:27:11,246 - experiments.base - INFO - 114/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:27:17,937 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:27:20,171 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:27:20,197 - experiments.base - INFO - 115/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:27:26,857 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:27:29,119 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:27:29,148 - experiments.base - INFO - 116/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:27:35,851 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:27:38,061 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:27:38,089 - experiments.base - INFO - 117/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:27:44,717 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:27:46,940 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:27:46,969 - experiments.base - INFO - 118/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:27:53,655 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:27:55,861 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:27:55,871 - experiments.base - INFO - 119/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:28:02,589 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:28:04,812 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:28:04,819 - experiments.base - INFO - 120/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:28:11,500 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:28:13,746 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:28:13,756 - experiments.base - INFO - 121/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:28:46,970 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:28:49,211 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:28:49,220 - experiments.base - INFO - 122/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:28:52,476 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:28:53,611 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:28:53,624 - experiments.base - INFO - 123/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:28:56,811 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:28:57,996 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:28:58,009 - experiments.base - INFO - 124/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:29:01,171 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:29:02,309 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:29:02,322 - experiments.base - INFO - 125/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:29:05,516 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:29:06,653 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:29:06,667 - experiments.base - INFO - 126/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:29:09,832 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:29:11,052 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:29:11,065 - experiments.base - INFO - 127/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:29:14,367 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:29:15,493 - experiments.base - INFO - reward_mean: 0.03125, reward_median: 0.03125, reward_std: 0.0, reward_max: 0.03125, reward_min: 0.03125, runs: 100
2019-04-12 00:29:15,503 - experiments.base - INFO - 128/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:29:18,651 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:29:19,868 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:29:19,882 - experiments.base - INFO - 129/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:29:23,226 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:29:24,367 - experiments.base - INFO - reward_mean: 0.03125, reward_median: 0.03125, reward_std: 0.0, reward_max: 0.03125, reward_min: 0.03125, runs: 100
2019-04-12 00:29:24,377 - experiments.base - INFO - 130/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:29:27,825 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:29:28,980 - experiments.base - INFO - reward_mean: 0.03125, reward_median: 0.03125, reward_std: 0.0, reward_max: 0.03125, reward_min: 0.03125, runs: 100
2019-04-12 00:29:28,990 - experiments.base - INFO - 131/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:30:02,142 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:30:04,424 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:30:04,434 - experiments.base - INFO - 132/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:30:07,756 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:30:08,897 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:30:08,911 - experiments.base - INFO - 133/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:30:12,184 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:30:13,315 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:30:13,329 - experiments.base - INFO - 134/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:30:16,628 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:30:17,832 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:30:17,845 - experiments.base - INFO - 135/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:30:21,105 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:30:22,253 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:30:22,266 - experiments.base - INFO - 136/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:30:25,496 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:30:26,630 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:30:26,644 - experiments.base - INFO - 137/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:30:29,890 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:30:31,091 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:30:31,107 - experiments.base - INFO - 138/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:30:34,490 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:30:35,684 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:30:35,700 - experiments.base - INFO - 139/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:30:38,973 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:30:40,157 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:30:40,174 - experiments.base - INFO - 140/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:30:43,509 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:30:44,661 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:30:44,674 - experiments.base - INFO - 141/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:31:17,825 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:31:20,075 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:31:20,085 - experiments.base - INFO - 142/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:31:23,476 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:31:24,655 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:31:24,671 - experiments.base - INFO - 143/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:31:28,220 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:31:29,355 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:31:29,371 - experiments.base - INFO - 144/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:31:32,732 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:31:33,858 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:31:33,872 - experiments.base - INFO - 145/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:31:37,278 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:31:38,470 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:31:38,483 - experiments.base - INFO - 146/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:31:41,828 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:31:42,964 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:31:42,980 - experiments.base - INFO - 147/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:31:46,319 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:31:47,438 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:31:47,453 - experiments.base - INFO - 148/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:31:50,894 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:31:52,079 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:31:52,095 - experiments.base - INFO - 149/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:31:55,467 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:31:56,614 - experiments.base - INFO - reward_mean: 0.033333333333333326, reward_median: 0.03333333333333333, reward_std: 6.938893903907228e-18, reward_max: 0.03333333333333333, reward_min: 0.03333333333333333, runs: 100
2019-04-12 00:31:56,630 - experiments.base - INFO - 150/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:32:00,151 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:32:01,335 - experiments.base - INFO - reward_mean: 0.03125, reward_median: 0.03125, reward_std: 0.0, reward_max: 0.03125, reward_min: 0.03125, runs: 100
2019-04-12 00:32:01,346 - experiments.base - INFO - 151/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:32:08,049 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:32:10,265 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:32:10,273 - experiments.base - INFO - 152/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:32:16,917 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:32:19,131 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:32:19,141 - experiments.base - INFO - 153/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:32:25,811 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:32:28,098 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:32:28,105 - experiments.base - INFO - 154/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:32:34,736 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:32:36,963 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:32:36,973 - experiments.base - INFO - 155/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:32:43,624 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:32:45,802 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:32:45,819 - experiments.base - INFO - 156/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:32:52,500 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:32:54,746 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:32:54,756 - experiments.base - INFO - 157/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:33:01,440 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:33:03,655 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:33:03,665 - experiments.base - INFO - 158/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:33:10,382 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:33:12,582 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:33:12,611 - experiments.base - INFO - 159/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:33:19,273 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:33:21,551 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:33:21,559 - experiments.base - INFO - 160/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:33:28,217 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:33:30,427 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:33:30,438 - experiments.base - INFO - 161/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:33:37,105 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:33:39,346 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:33:39,355 - experiments.base - INFO - 162/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:33:45,990 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:33:48,203 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:33:48,211 - experiments.base - INFO - 163/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:33:54,943 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:33:57,122 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:33:57,132 - experiments.base - INFO - 164/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:34:03,785 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:34:06,055 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:34:06,065 - experiments.base - INFO - 165/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:34:12,657 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:34:14,882 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:34:14,907 - experiments.base - INFO - 166/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:34:21,601 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:34:23,790 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:34:23,816 - experiments.base - INFO - 167/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:34:30,483 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:34:32,713 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:34:32,739 - experiments.base - INFO - 168/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:34:39,430 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:34:41,642 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:34:41,651 - experiments.base - INFO - 169/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:34:48,323 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:34:50,565 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:34:50,575 - experiments.base - INFO - 170/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:34:57,240 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:34:59,494 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:34:59,503 - experiments.base - INFO - 171/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:35:06,210 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:35:08,414 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:35:08,424 - experiments.base - INFO - 172/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:35:15,151 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:35:17,401 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:35:17,411 - experiments.base - INFO - 173/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:35:24,104 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:35:26,296 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:35:26,305 - experiments.base - INFO - 174/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:35:32,963 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:35:35,191 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:35:35,201 - experiments.base - INFO - 175/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:35:41,904 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:35:44,151 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:35:44,160 - experiments.base - INFO - 176/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:35:50,842 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:35:53,038 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:35:53,046 - experiments.base - INFO - 177/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:35:59,726 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:36:01,924 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:36:01,934 - experiments.base - INFO - 178/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:36:08,668 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:36:10,910 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:36:10,920 - experiments.base - INFO - 179/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:36:17,648 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:36:19,881 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:36:19,890 - experiments.base - INFO - 180/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:36:26,546 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:36:28,726 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:36:28,736 - __main__ - INFO - Running Q experiment: Mazeworld (8x8)
2019-04-12 00:36:28,742 - experiments.base - INFO - Searching Q in 180 dimensions
2019-04-12 00:36:28,742 - experiments.base - INFO - 1/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:36:50,470 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:36:52,440 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:36:52,447 - experiments.base - INFO - 2/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:36:54,917 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:36:55,743 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:36:55,756 - experiments.base - INFO - 3/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:36:58,020 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:36:58,841 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:36:58,852 - experiments.base - INFO - 4/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:37:00,704 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:37:01,486 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:37:01,499 - experiments.base - INFO - 5/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:37:03,301 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:37:04,073 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:37:04,085 - experiments.base - INFO - 6/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:37:05,934 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:37:06,753 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:37:06,766 - experiments.base - INFO - 7/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:37:08,502 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:37:09,289 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:37:09,301 - experiments.base - INFO - 8/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:37:11,344 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:37:12,128 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:37:12,141 - experiments.base - INFO - 9/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:37:13,983 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:37:14,898 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:37:14,911 - experiments.base - INFO - 10/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:37:17,374 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:37:18,164 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:37:18,180 - experiments.base - INFO - 11/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:37:39,855 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:37:41,769 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:37:41,778 - experiments.base - INFO - 12/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:37:44,299 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:37:45,121 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:37:45,132 - experiments.base - INFO - 13/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:37:47,286 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:37:48,056 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:37:48,069 - experiments.base - INFO - 14/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:37:50,092 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:37:50,910 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:37:50,921 - experiments.base - INFO - 15/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:37:52,826 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:37:53,601 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:37:53,612 - experiments.base - INFO - 16/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:37:55,385 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:37:56,164 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:37:56,177 - experiments.base - INFO - 17/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:37:57,970 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:37:58,783 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:37:58,796 - experiments.base - INFO - 18/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:38:00,533 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:38:01,329 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:38:01,345 - experiments.base - INFO - 19/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:38:03,382 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:38:04,273 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:38:04,286 - experiments.base - INFO - 20/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:38:06,059 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:38:06,887 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:38:06,898 - experiments.base - INFO - 21/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:38:29,878 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:38:31,818 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:38:31,826 - experiments.base - INFO - 22/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:38:34,447 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:38:35,247 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:38:35,263 - experiments.base - INFO - 23/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:38:37,469 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:38:38,286 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:38:38,299 - experiments.base - INFO - 24/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:38:40,308 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:38:41,073 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:38:41,086 - experiments.base - INFO - 25/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:38:42,992 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:38:43,848 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:38:43,861 - experiments.base - INFO - 26/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:38:45,799 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:38:46,578 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:38:46,591 - experiments.base - INFO - 27/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:38:48,484 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:38:49,263 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:38:49,273 - experiments.base - INFO - 28/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:38:51,157 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:38:51,984 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:38:51,999 - experiments.base - INFO - 29/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:38:53,885 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:38:54,687 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:38:54,700 - experiments.base - INFO - 30/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:38:56,638 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:38:57,415 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:38:57,427 - experiments.base - INFO - 31/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:39:04,072 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:39:05,996 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:39:06,005 - experiments.base - INFO - 32/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:39:12,594 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:39:14,469 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:39:14,476 - experiments.base - INFO - 33/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:39:21,108 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:39:22,974 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:39:22,983 - experiments.base - INFO - 34/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:39:29,582 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:39:31,463 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:39:31,473 - experiments.base - INFO - 35/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:39:38,078 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:39:39,950 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:39:39,960 - experiments.base - INFO - 36/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:39:46,545 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:39:48,417 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:39:48,428 - experiments.base - INFO - 37/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:39:55,007 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:39:56,861 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:39:56,871 - experiments.base - INFO - 38/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:40:03,470 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:40:05,329 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:40:05,338 - experiments.base - INFO - 39/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:40:11,887 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:40:13,788 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:40:13,796 - experiments.base - INFO - 40/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:40:20,476 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:40:22,336 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:40:22,345 - experiments.base - INFO - 41/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:40:28,900 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:40:30,737 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:40:30,746 - experiments.base - INFO - 42/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:40:37,341 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:40:39,221 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:40:39,233 - experiments.base - INFO - 43/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:40:45,825 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:40:47,677 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:40:47,687 - experiments.base - INFO - 44/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:40:54,276 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:40:56,134 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:40:56,145 - experiments.base - INFO - 45/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:41:02,707 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:41:04,575 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:41:04,585 - experiments.base - INFO - 46/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:41:11,210 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:41:13,073 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:41:13,085 - experiments.base - INFO - 47/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:41:19,717 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:41:21,621 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:41:21,631 - experiments.base - INFO - 48/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:41:28,239 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:41:30,076 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:41:30,085 - experiments.base - INFO - 49/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:41:36,671 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:41:38,519 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:41:38,528 - experiments.base - INFO - 50/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:41:45,118 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:41:47,006 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:41:47,013 - experiments.base - INFO - 51/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:41:53,642 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:41:55,483 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:41:55,492 - experiments.base - INFO - 52/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:42:02,082 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:42:03,966 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:42:03,974 - experiments.base - INFO - 53/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:42:10,609 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:42:12,510 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:42:12,522 - experiments.base - INFO - 54/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:42:19,125 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:42:21,007 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:42:21,017 - experiments.base - INFO - 55/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:42:27,599 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:42:29,438 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:42:29,447 - experiments.base - INFO - 56/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:42:36,073 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:42:37,967 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:42:37,979 - experiments.base - INFO - 57/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:42:44,542 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:42:46,382 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:42:46,394 - experiments.base - INFO - 58/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:42:53,007 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:42:54,907 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:42:54,917 - experiments.base - INFO - 59/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:43:01,440 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:43:03,305 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:43:03,319 - experiments.base - INFO - 60/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:43:09,926 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:43:11,789 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:43:11,796 - experiments.base - INFO - 61/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:43:40,953 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:43:42,878 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:43:42,888 - experiments.base - INFO - 62/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:43:44,519 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:43:45,286 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:43:45,299 - experiments.base - INFO - 63/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:43:46,881 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:43:47,648 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:43:47,661 - experiments.base - INFO - 64/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:43:49,233 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:43:50,084 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:43:50,096 - experiments.base - INFO - 65/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:43:51,654 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:43:52,433 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:43:52,446 - experiments.base - INFO - 66/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:43:54,051 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:43:54,825 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:43:54,838 - experiments.base - INFO - 67/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:43:56,410 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:43:57,224 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:43:57,237 - experiments.base - INFO - 68/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:43:58,737 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:43:59,513 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:43:59,526 - experiments.base - INFO - 69/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:44:01,115 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:44:01,934 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:44:01,950 - experiments.base - INFO - 70/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:44:03,525 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:44:04,315 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:44:04,328 - experiments.base - INFO - 71/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:44:33,555 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:44:35,466 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:44:35,474 - experiments.base - INFO - 72/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:44:37,170 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:44:37,997 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:44:38,012 - experiments.base - INFO - 73/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:44:39,684 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:44:40,467 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:44:40,480 - experiments.base - INFO - 74/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:44:42,134 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:44:42,914 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:44:42,927 - experiments.base - INFO - 75/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:44:44,592 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:44:45,411 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:44:45,424 - experiments.base - INFO - 76/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:44:47,078 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:44:47,858 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:44:47,871 - experiments.base - INFO - 77/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:44:49,456 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:44:50,267 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:44:50,279 - experiments.base - INFO - 78/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:44:51,950 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:44:52,788 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:44:52,802 - experiments.base - INFO - 79/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:44:54,526 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:44:55,306 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:44:55,319 - experiments.base - INFO - 80/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:44:56,980 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:44:57,796 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:44:57,809 - experiments.base - INFO - 81/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:45:27,061 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:45:28,956 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:45:28,964 - experiments.base - INFO - 82/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:45:30,763 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:45:31,546 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:45:31,562 - experiments.base - INFO - 83/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:45:33,319 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:45:34,134 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:45:34,145 - experiments.base - INFO - 84/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:45:35,944 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:45:36,736 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:45:36,749 - experiments.base - INFO - 85/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:45:38,493 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:45:39,269 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:45:39,283 - experiments.base - INFO - 86/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:45:41,032 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:45:41,855 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:45:41,869 - experiments.base - INFO - 87/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:45:43,615 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:45:44,401 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:45:44,417 - experiments.base - INFO - 88/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:45:46,194 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:45:47,015 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:45:47,029 - experiments.base - INFO - 89/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:45:48,766 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:45:49,589 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:45:49,605 - experiments.base - INFO - 90/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:45:51,398 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:45:52,204 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:45:52,217 - experiments.base - INFO - 91/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:45:58,812 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:46:00,713 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:46:00,721 - experiments.base - INFO - 92/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:46:07,292 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:46:09,148 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:46:09,157 - experiments.base - INFO - 93/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:46:15,706 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:46:17,561 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:46:17,569 - experiments.base - INFO - 94/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:46:24,168 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:46:26,049 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:46:26,059 - experiments.base - INFO - 95/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:46:32,769 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:46:34,617 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:46:34,625 - experiments.base - INFO - 96/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:46:41,226 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:46:43,085 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:46:43,094 - experiments.base - INFO - 97/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:46:49,681 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:46:51,604 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:46:51,612 - experiments.base - INFO - 98/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:46:58,181 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:47:00,028 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:47:00,036 - experiments.base - INFO - 99/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:47:06,650 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:47:08,500 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:47:08,507 - experiments.base - INFO - 100/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:47:15,099 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:47:16,976 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:47:16,984 - experiments.base - INFO - 101/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:47:23,601 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:47:25,453 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:47:25,463 - experiments.base - INFO - 102/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:47:32,032 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:47:33,927 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:47:33,937 - experiments.base - INFO - 103/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:47:40,532 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:47:42,371 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:47:42,381 - experiments.base - INFO - 104/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:47:48,941 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:47:50,811 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:47:50,822 - experiments.base - INFO - 105/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:47:57,384 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:47:59,266 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:47:59,279 - experiments.base - INFO - 106/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:48:05,914 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:48:07,767 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:48:07,779 - experiments.base - INFO - 107/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:48:14,375 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:48:16,226 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:48:16,236 - experiments.base - INFO - 108/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:48:22,894 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:48:24,779 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:48:24,789 - experiments.base - INFO - 109/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:48:31,351 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:48:33,204 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:48:33,216 - experiments.base - INFO - 110/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:48:39,801 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:48:41,658 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:48:41,670 - experiments.base - INFO - 111/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:48:48,246 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:48:50,167 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:48:50,176 - experiments.base - INFO - 112/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:48:56,744 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:48:58,595 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:48:58,605 - experiments.base - INFO - 113/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:49:05,203 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:49:07,085 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:49:07,098 - experiments.base - INFO - 114/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:49:13,668 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:49:15,520 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:49:15,529 - experiments.base - INFO - 115/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:49:22,105 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:49:23,947 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:49:23,957 - experiments.base - INFO - 116/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:49:30,506 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:49:32,378 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:49:32,394 - experiments.base - INFO - 117/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:49:38,994 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:49:40,887 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:49:40,895 - experiments.base - INFO - 118/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:49:47,447 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:49:49,290 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:49:49,299 - experiments.base - INFO - 119/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:49:55,941 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:49:57,831 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:49:57,839 - experiments.base - INFO - 120/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:50:04,420 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:50:06,301 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:50:06,309 - experiments.base - INFO - 121/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:50:37,878 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:50:39,762 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:50:39,770 - experiments.base - INFO - 122/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:50:41,313 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:50:42,131 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:50:42,145 - experiments.base - INFO - 123/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:50:43,677 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:50:44,453 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:50:44,467 - experiments.base - INFO - 124/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:50:45,983 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:50:46,811 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:50:46,823 - experiments.base - INFO - 125/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:50:48,361 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:50:49,134 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:50:49,148 - experiments.base - INFO - 126/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:50:50,729 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:50:51,502 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:50:51,516 - experiments.base - INFO - 127/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:50:53,062 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:50:53,880 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:50:53,894 - experiments.base - INFO - 128/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:50:55,424 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:50:56,201 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:50:56,216 - experiments.base - INFO - 129/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:50:57,719 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:50:58,499 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:50:58,510 - experiments.base - INFO - 130/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:51:00,059 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:51:00,880 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:51:00,892 - experiments.base - INFO - 131/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:51:32,671 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:51:34,561 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:51:34,569 - experiments.base - INFO - 132/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:51:36,220 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:51:37,012 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:51:37,025 - experiments.base - INFO - 133/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:51:38,644 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:51:39,471 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:51:39,486 - experiments.base - INFO - 134/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:51:41,128 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:51:41,898 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:51:41,913 - experiments.base - INFO - 135/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:51:43,792 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:51:44,674 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:51:44,687 - experiments.base - INFO - 136/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:51:46,344 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:51:47,135 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:51:47,150 - experiments.base - INFO - 137/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:51:48,780 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:51:49,555 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:51:49,569 - experiments.base - INFO - 138/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:51:51,240 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:51:52,079 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:51:52,094 - experiments.base - INFO - 139/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:51:53,746 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:51:54,528 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:51:54,542 - experiments.base - INFO - 140/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:51:56,164 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:51:56,940 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:51:56,953 - experiments.base - INFO - 141/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:52:28,459 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:52:30,392 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:52:30,401 - experiments.base - INFO - 142/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:52:32,148 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:52:32,921 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:52:32,937 - experiments.base - INFO - 143/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:52:34,680 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:52:35,492 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:52:35,506 - experiments.base - INFO - 144/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:52:37,198 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:52:38,025 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:52:38,039 - experiments.base - INFO - 145/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:52:39,775 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:52:40,572 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:52:40,585 - experiments.base - INFO - 146/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:52:42,332 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:52:43,171 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:52:43,186 - experiments.base - INFO - 147/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:52:44,928 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:52:45,703 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:52:45,719 - experiments.base - INFO - 148/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:52:47,417 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:52:48,207 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:52:48,223 - experiments.base - INFO - 149/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:52:49,979 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:52:50,805 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:52:50,822 - experiments.base - INFO - 150/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:52:52,548 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:52:53,323 - experiments.base - INFO - reward_mean: 0.0714285714285714, reward_median: 0.07142857142857142, reward_std: 2.7755575615628914e-17, reward_max: 0.07142857142857142, reward_min: 0.07142857142857142, runs: 100
2019-04-12 00:52:53,338 - experiments.base - INFO - 151/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:52:59,963 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:53:01,828 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:53:01,838 - experiments.base - INFO - 152/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:53:08,490 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:53:10,450 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:53:10,459 - experiments.base - INFO - 153/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:53:17,023 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:53:18,943 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:53:18,951 - experiments.base - INFO - 154/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:53:25,569 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:53:27,407 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:53:27,415 - experiments.base - INFO - 155/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:53:33,928 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:53:35,848 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:53:35,858 - experiments.base - INFO - 156/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:53:42,359 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:53:44,233 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:53:44,242 - experiments.base - INFO - 157/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:53:50,746 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:53:52,618 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:53:52,628 - experiments.base - INFO - 158/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:53:59,112 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:54:00,961 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:54:00,970 - experiments.base - INFO - 159/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:54:07,628 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:54:09,460 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:54:09,470 - experiments.base - INFO - 160/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:54:16,023 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:54:17,901 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:54:17,910 - experiments.base - INFO - 161/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:54:24,463 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:54:26,313 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:54:26,323 - experiments.base - INFO - 162/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:54:32,920 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:54:34,770 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:54:34,780 - experiments.base - INFO - 163/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:54:41,345 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:54:43,221 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:54:43,233 - experiments.base - INFO - 164/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:54:49,746 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:54:51,604 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:54:51,612 - experiments.base - INFO - 165/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:54:58,147 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:54:59,993 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:55:00,002 - experiments.base - INFO - 166/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:55:06,515 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:55:08,392 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:55:08,401 - experiments.base - INFO - 167/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:55:14,903 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:55:16,746 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:55:16,756 - experiments.base - INFO - 168/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:55:23,328 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:55:25,193 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:55:25,201 - experiments.base - INFO - 169/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:55:31,688 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:55:33,530 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:55:33,540 - experiments.base - INFO - 170/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:55:40,065 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:55:41,907 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:55:41,917 - experiments.base - INFO - 171/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:55:48,471 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:55:50,394 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:55:50,404 - experiments.base - INFO - 172/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:55:56,885 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:55:58,733 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:55:58,744 - experiments.base - INFO - 173/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:56:05,272 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:56:07,117 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:56:07,127 - experiments.base - INFO - 174/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:56:13,707 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:56:15,634 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:56:15,641 - experiments.base - INFO - 175/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:56:22,217 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:56:24,072 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:56:24,082 - experiments.base - INFO - 176/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:56:30,647 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:56:32,503 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:56:32,515 - experiments.base - INFO - 177/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:56:39,051 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:56:40,915 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:56:40,924 - experiments.base - INFO - 178/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:56:47,473 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:56:49,369 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:56:49,384 - experiments.base - INFO - 179/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:56:55,941 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:56:57,832 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:56:57,842 - experiments.base - INFO - 180/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:57:04,413 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:57:06,286 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:57:06,296 - __main__ - INFO - Running Q experiment: Mazeworld (9x9)
2019-04-12 00:57:06,299 - experiments.base - INFO - Searching Q in 180 dimensions
2019-04-12 00:57:06,299 - experiments.base - INFO - 1/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:57:39,460 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:57:41,443 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:57:41,450 - experiments.base - INFO - 2/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:57:50,966 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:57:51,917 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 00:57:51,954 - experiments.base - INFO - 3/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:57:59,868 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:58:00,812 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 00:58:00,848 - experiments.base - INFO - 4/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:58:08,214 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:58:09,128 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 00:58:09,167 - experiments.base - INFO - 5/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:58:15,947 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:58:16,911 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 00:58:16,947 - experiments.base - INFO - 6/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:58:23,562 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:58:24,503 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 00:58:24,540 - experiments.base - INFO - 7/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:58:30,908 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:58:31,815 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 00:58:31,851 - experiments.base - INFO - 8/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:58:37,770 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:58:38,727 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 00:58:38,765 - experiments.base - INFO - 9/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 00:58:44,920 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:58:45,832 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 00:58:45,868 - experiments.base - INFO - 10/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 00:58:52,631 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:58:53,611 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 00:58:53,650 - experiments.base - INFO - 11/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 00:59:00,207 - experiments.base - INFO - Took 200 episodes
2019-04-12 00:59:02,158 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 00:59:02,167 - experiments.base - INFO - 12/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 00:59:11,529 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:59:12,447 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 00:59:12,484 - experiments.base - INFO - 13/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 00:59:20,529 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:59:21,490 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 00:59:21,526 - experiments.base - INFO - 14/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 00:59:28,908 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:59:29,812 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 00:59:29,851 - experiments.base - INFO - 15/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 00:59:36,520 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:59:37,434 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 00:59:37,473 - experiments.base - INFO - 16/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 00:59:43,773 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:59:44,730 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 00:59:44,769 - experiments.base - INFO - 17/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 00:59:50,746 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:59:51,657 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 00:59:51,697 - experiments.base - INFO - 18/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 00:59:57,720 - experiments.base - INFO - Took 1000 episodes
2019-04-12 00:59:58,631 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 00:59:58,671 - experiments.base - INFO - 19/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:00:04,792 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:00:05,769 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:00:05,809 - experiments.base - INFO - 20/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:00:11,210 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:00:12,131 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:00:12,170 - experiments.base - INFO - 21/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:00:44,628 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:00:46,651 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:00:46,661 - experiments.base - INFO - 22/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:00:56,216 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:00:57,145 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:00:57,181 - experiments.base - INFO - 23/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:01:05,267 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:01:06,184 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:01:06,220 - experiments.base - INFO - 24/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:01:13,459 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:01:14,418 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:01:14,457 - experiments.base - INFO - 25/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:01:21,365 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:01:22,282 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:01:22,318 - experiments.base - INFO - 26/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:01:29,066 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:01:29,971 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:01:30,007 - experiments.base - INFO - 27/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:01:36,619 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:01:37,612 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:01:37,650 - experiments.base - INFO - 28/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:01:43,831 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:01:44,763 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:01:44,802 - experiments.base - INFO - 29/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:01:50,555 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:01:51,477 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:01:51,516 - experiments.base - INFO - 30/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:01:57,581 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:01:58,535 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:01:58,573 - experiments.base - INFO - 31/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:02:05,184 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:02:07,134 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:02:07,145 - experiments.base - INFO - 32/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:02:13,691 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:02:15,667 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:02:15,677 - experiments.base - INFO - 33/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:02:22,286 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:02:24,243 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:02:24,253 - experiments.base - INFO - 34/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:02:30,790 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:02:32,713 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:02:32,723 - experiments.base - INFO - 35/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:02:39,334 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:02:41,332 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:02:41,342 - experiments.base - INFO - 36/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:02:47,944 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:02:49,904 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:02:49,911 - experiments.base - INFO - 37/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:02:56,467 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:02:58,430 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:02:58,440 - experiments.base - INFO - 38/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:03:05,121 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:03:07,128 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:03:07,137 - experiments.base - INFO - 39/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:03:13,680 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:03:15,625 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:03:15,632 - experiments.base - INFO - 40/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:03:22,187 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:03:24,128 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:03:24,138 - experiments.base - INFO - 41/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:03:30,631 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:03:32,609 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:03:32,618 - experiments.base - INFO - 42/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:03:39,174 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:03:41,122 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:03:41,134 - experiments.base - INFO - 43/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:03:47,677 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:03:49,642 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:03:49,651 - experiments.base - INFO - 44/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:03:56,194 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:03:58,128 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:03:58,138 - experiments.base - INFO - 45/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:04:04,709 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:04:06,634 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:04:06,648 - experiments.base - INFO - 46/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:04:13,161 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:04:15,171 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:04:15,180 - experiments.base - INFO - 47/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:04:21,762 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:04:23,696 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:04:23,704 - experiments.base - INFO - 48/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:04:30,278 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:04:32,204 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:04:32,213 - experiments.base - INFO - 49/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:04:38,766 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:04:40,734 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:04:40,743 - experiments.base - INFO - 50/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:04:47,240 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:04:49,157 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:04:49,165 - experiments.base - INFO - 51/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:04:55,687 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:04:57,609 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:04:57,618 - experiments.base - INFO - 52/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:05:04,161 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:05:06,151 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:05:06,161 - experiments.base - INFO - 53/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:05:12,703 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:05:14,663 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:05:14,671 - experiments.base - INFO - 54/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:05:21,249 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:05:23,249 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:05:23,256 - experiments.base - INFO - 55/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:05:29,767 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:05:31,707 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:05:31,717 - experiments.base - INFO - 56/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:05:38,263 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:05:40,211 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:05:40,220 - experiments.base - INFO - 57/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:05:46,752 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:05:48,729 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:05:48,736 - experiments.base - INFO - 58/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:05:55,276 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:05:57,217 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:05:57,226 - experiments.base - INFO - 59/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:06:03,762 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:06:05,747 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:06:05,756 - experiments.base - INFO - 60/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:06:12,299 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:06:14,301 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:06:14,309 - experiments.base - INFO - 61/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:06:47,303 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:06:49,282 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:06:49,290 - experiments.base - INFO - 62/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:06:53,943 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:06:54,842 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:06:54,881 - experiments.base - INFO - 63/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:06:59,278 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:07:00,230 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:07:00,267 - experiments.base - INFO - 64/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:07:04,667 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:07:05,625 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:07:05,664 - experiments.base - INFO - 65/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:07:09,920 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:07:10,845 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:07:10,882 - experiments.base - INFO - 66/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:07:15,105 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:07:16,006 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:07:16,045 - experiments.base - INFO - 67/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:07:20,335 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:07:21,275 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:07:21,312 - experiments.base - INFO - 68/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:07:25,530 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:07:26,453 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:07:26,490 - experiments.base - INFO - 69/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:07:30,529 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:07:31,434 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:07:31,473 - experiments.base - INFO - 70/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:07:35,812 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:07:36,733 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:07:36,772 - experiments.base - INFO - 71/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:08:09,595 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:08:11,645 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:08:11,654 - experiments.base - INFO - 72/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:08:16,398 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:08:17,312 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:08:17,352 - experiments.base - INFO - 73/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:08:21,878 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:08:22,780 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:08:22,819 - experiments.base - INFO - 74/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:08:27,180 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:08:28,118 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:08:28,154 - experiments.base - INFO - 75/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:08:32,447 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:08:33,339 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:08:33,377 - experiments.base - INFO - 76/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:08:37,677 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:08:38,614 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:08:38,653 - experiments.base - INFO - 77/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:08:42,977 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:08:43,872 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:08:43,911 - experiments.base - INFO - 78/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:08:48,148 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:08:49,039 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:08:49,078 - experiments.base - INFO - 79/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:08:53,384 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:08:54,345 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:08:54,385 - experiments.base - INFO - 80/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:08:58,648 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:08:59,551 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:08:59,591 - experiments.base - INFO - 81/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:09:32,421 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:09:34,378 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:09:34,388 - experiments.base - INFO - 82/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:09:39,071 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:09:40,029 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:09:40,065 - experiments.base - INFO - 83/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:09:44,634 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:09:45,526 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:09:45,565 - experiments.base - INFO - 84/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:09:50,061 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:09:50,970 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:09:51,009 - experiments.base - INFO - 85/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:09:55,380 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:09:56,316 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:09:56,354 - experiments.base - INFO - 86/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:10:00,730 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:10:01,615 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:10:01,654 - experiments.base - INFO - 87/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:10:05,986 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:10:06,927 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:10:06,967 - experiments.base - INFO - 88/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:10:11,329 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:10:12,224 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:10:12,263 - experiments.base - INFO - 89/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:10:16,565 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:10:17,474 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:10:17,515 - experiments.base - INFO - 90/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:10:21,841 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:10:22,805 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:10:22,842 - experiments.base - INFO - 91/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:10:29,381 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:10:31,306 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:10:31,315 - experiments.base - INFO - 92/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:10:37,875 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:10:39,880 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:10:39,888 - experiments.base - INFO - 93/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:10:46,460 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:10:48,483 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:10:48,493 - experiments.base - INFO - 94/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:10:55,033 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:10:56,976 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:10:56,986 - experiments.base - INFO - 95/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:11:03,559 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:11:05,533 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:11:05,542 - experiments.base - INFO - 96/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:11:12,085 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:11:14,072 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:11:14,078 - experiments.base - INFO - 97/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:11:20,668 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:11:22,644 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:11:22,653 - experiments.base - INFO - 98/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:11:29,369 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:11:31,326 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:11:31,335 - experiments.base - INFO - 99/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:11:37,867 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:11:39,795 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:11:39,803 - experiments.base - INFO - 100/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:11:46,369 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:11:48,283 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:11:48,292 - experiments.base - INFO - 101/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:11:54,861 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:11:56,846 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:11:56,855 - experiments.base - INFO - 102/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:12:03,388 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:12:05,381 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:12:05,391 - experiments.base - INFO - 103/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:12:11,957 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:12:13,904 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:12:13,914 - experiments.base - INFO - 104/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:12:20,503 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:12:22,499 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:12:22,509 - experiments.base - INFO - 105/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:12:29,088 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:12:31,022 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:12:31,032 - experiments.base - INFO - 106/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:12:37,565 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:12:39,483 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:12:39,492 - experiments.base - INFO - 107/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:12:46,059 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:12:48,036 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:12:48,045 - experiments.base - INFO - 108/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:12:54,624 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:12:56,565 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:12:56,575 - experiments.base - INFO - 109/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:13:03,086 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:13:05,101 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:13:05,111 - experiments.base - INFO - 110/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:13:11,719 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:13:13,671 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:13:13,680 - experiments.base - INFO - 111/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:13:20,279 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:13:22,216 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:13:22,224 - experiments.base - INFO - 112/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:13:28,779 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:13:30,756 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:13:30,766 - experiments.base - INFO - 113/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:13:37,313 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:13:39,239 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:13:39,246 - experiments.base - INFO - 114/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:13:45,792 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:13:47,726 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:13:47,736 - experiments.base - INFO - 115/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:13:54,326 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:13:56,285 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:13:56,293 - experiments.base - INFO - 116/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:14:02,875 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:14:04,802 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:14:04,812 - experiments.base - INFO - 117/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:14:11,397 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:14:13,348 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:14:13,355 - experiments.base - INFO - 118/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:14:19,880 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:14:21,884 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:14:21,901 - experiments.base - INFO - 119/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:14:28,417 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:14:30,349 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:14:30,358 - experiments.base - INFO - 120/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:14:36,928 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:14:38,901 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:14:38,910 - experiments.base - INFO - 121/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:14:45,427 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:14:47,378 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:14:47,387 - experiments.base - INFO - 122/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:14:51,489 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:14:52,391 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:14:52,431 - experiments.base - INFO - 123/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:14:56,480 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:14:57,404 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:14:57,441 - experiments.base - INFO - 124/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:15:01,464 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:15:02,362 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:15:02,401 - experiments.base - INFO - 125/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:15:06,558 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:15:07,464 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:15:07,503 - experiments.base - INFO - 126/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:15:11,477 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:15:12,398 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:15:12,436 - experiments.base - INFO - 127/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:15:16,444 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:15:17,329 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:15:17,368 - experiments.base - INFO - 128/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:15:21,403 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:15:22,315 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:15:22,355 - experiments.base - INFO - 129/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:15:26,448 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:15:27,385 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:15:27,421 - experiments.base - INFO - 130/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:15:31,510 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:15:32,433 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:15:32,471 - experiments.base - INFO - 131/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:16:05,029 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:16:07,062 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:16:07,072 - experiments.base - INFO - 132/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:16:11,209 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:16:12,098 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:16:12,137 - experiments.base - INFO - 133/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:16:16,296 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:16:17,193 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:16:17,232 - experiments.base - INFO - 134/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:16:21,433 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:16:22,374 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:16:22,411 - experiments.base - INFO - 135/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:16:26,512 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:16:27,413 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:16:27,450 - experiments.base - INFO - 136/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:16:31,512 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:16:32,411 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:16:32,450 - experiments.base - INFO - 137/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:16:36,546 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:16:37,489 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:16:37,528 - experiments.base - INFO - 138/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:16:41,584 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:16:42,493 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:16:42,532 - experiments.base - INFO - 139/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:16:46,653 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:16:47,552 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:16:47,594 - experiments.base - INFO - 140/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:16:51,782 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:16:52,740 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:16:52,779 - experiments.base - INFO - 141/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:16:59,316 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:17:01,265 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:17:01,273 - experiments.base - INFO - 142/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:17:05,513 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:17:06,447 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:17:06,484 - experiments.base - INFO - 143/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:17:10,648 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:17:11,546 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:17:11,586 - experiments.base - INFO - 144/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:17:15,983 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:17:16,963 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:17:17,002 - experiments.base - INFO - 145/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:17:21,315 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:17:22,269 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:17:22,315 - experiments.base - INFO - 146/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:17:26,542 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:17:27,426 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:17:27,463 - experiments.base - INFO - 147/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:17:31,690 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:17:32,588 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:17:32,635 - experiments.base - INFO - 148/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:17:36,877 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:17:37,829 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:17:37,869 - experiments.base - INFO - 149/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:17:42,134 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:17:43,040 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:17:43,076 - experiments.base - INFO - 150/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:17:47,355 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:17:48,247 - experiments.base - INFO - reward_mean: 0.02083333333333334, reward_median: 0.020833333333333332, reward_std: 6.938893903907228e-18, reward_max: 0.020833333333333332, reward_min: 0.020833333333333332, runs: 100
2019-04-12 01:17:48,286 - experiments.base - INFO - 151/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:17:54,888 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:17:56,877 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:17:56,887 - experiments.base - INFO - 152/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:18:03,448 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:18:05,434 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:18:05,444 - experiments.base - INFO - 153/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:18:12,049 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:18:14,032 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:18:14,042 - experiments.base - INFO - 154/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:18:20,653 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:18:22,595 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:18:22,611 - experiments.base - INFO - 155/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:18:29,197 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:18:31,141 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:18:31,150 - experiments.base - INFO - 156/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:18:37,743 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:18:39,717 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:18:39,727 - experiments.base - INFO - 157/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:18:46,240 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:18:48,177 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:18:48,187 - experiments.base - INFO - 158/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:18:54,776 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:18:56,717 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:18:56,726 - experiments.base - INFO - 159/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:19:03,290 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:19:05,288 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:19:05,296 - experiments.base - INFO - 160/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:19:11,888 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:19:13,861 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:19:13,871 - experiments.base - INFO - 161/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:19:20,490 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:19:22,440 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:19:22,450 - experiments.base - INFO - 162/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:19:28,967 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:19:30,973 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:19:30,983 - experiments.base - INFO - 163/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:19:37,555 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:19:39,503 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:19:39,513 - experiments.base - INFO - 164/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:19:46,022 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:19:47,993 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:19:48,006 - experiments.base - INFO - 165/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:19:54,559 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:19:56,494 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:19:56,505 - experiments.base - INFO - 166/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:20:03,071 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:20:05,046 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:20:05,056 - experiments.base - INFO - 167/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:20:11,568 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:20:13,542 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:20:13,553 - experiments.base - INFO - 168/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:20:20,223 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:20:22,174 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:20:22,184 - experiments.base - INFO - 169/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:20:28,743 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:20:30,674 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:20:30,686 - experiments.base - INFO - 170/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:20:37,233 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:20:39,213 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:20:39,226 - experiments.base - INFO - 171/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:20:45,842 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:20:47,799 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:20:47,811 - experiments.base - INFO - 172/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:20:54,424 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:20:56,359 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:20:56,371 - experiments.base - INFO - 173/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:21:02,891 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:21:04,865 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:21:04,874 - experiments.base - INFO - 174/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:21:11,448 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:21:13,395 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:21:13,404 - experiments.base - INFO - 175/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:21:20,059 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:21:22,058 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:21:22,065 - experiments.base - INFO - 176/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:21:28,674 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:21:30,608 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:21:30,618 - experiments.base - INFO - 177/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:21:37,255 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:21:39,203 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:21:39,214 - experiments.base - INFO - 178/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:21:45,831 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:21:47,812 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:21:47,822 - experiments.base - INFO - 179/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:21:54,430 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:21:56,368 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:21:56,380 - experiments.base - INFO - 180/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:22:02,976 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:22:04,915 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:22:04,930 - __main__ - INFO - Running Q experiment: Mazeworld (15x15)
2019-04-12 01:22:04,938 - experiments.base - INFO - Searching Q in 180 dimensions
2019-04-12 01:22:04,938 - experiments.base - INFO - 1/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:22:11,713 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:22:14,691 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:22:14,698 - experiments.base - INFO - 2/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:22:23,384 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:22:25,243 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:22:25,257 - experiments.base - INFO - 3/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:22:32,852 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:22:34,772 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:22:34,782 - experiments.base - INFO - 4/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:22:41,552 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:22:43,414 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:22:43,424 - experiments.base - INFO - 5/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:22:49,872 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:22:51,778 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:22:51,788 - experiments.base - INFO - 6/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:22:58,721 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:23:00,582 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:23:00,592 - experiments.base - INFO - 7/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:23:06,516 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:23:08,369 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:23:08,378 - experiments.base - INFO - 8/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:23:14,385 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:23:16,259 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:23:16,267 - experiments.base - INFO - 9/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:23:22,632 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:23:24,486 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:23:24,494 - experiments.base - INFO - 10/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:23:30,157 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:23:32,046 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:23:32,053 - experiments.base - INFO - 11/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:24:05,650 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:24:08,604 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:24:08,611 - experiments.base - INFO - 12/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:24:17,128 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:24:19,015 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:24:19,023 - experiments.base - INFO - 13/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:24:26,380 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:24:28,230 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:24:28,239 - experiments.base - INFO - 14/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:24:35,325 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:24:37,213 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:24:37,221 - experiments.base - INFO - 15/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:24:43,757 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:24:45,615 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:24:45,622 - experiments.base - INFO - 16/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:24:51,808 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:24:53,696 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:24:53,703 - experiments.base - INFO - 17/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:24:59,798 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:25:01,641 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:25:01,650 - experiments.base - INFO - 18/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:25:07,525 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:25:09,391 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:25:09,398 - experiments.base - INFO - 19/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:25:14,976 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:25:16,901 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:25:16,911 - experiments.base - INFO - 20/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:25:22,608 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:25:24,461 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:25:24,471 - experiments.base - INFO - 21/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:25:57,766 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:26:00,716 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:26:00,726 - experiments.base - INFO - 22/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:26:09,144 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:26:10,993 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:26:11,006 - experiments.base - INFO - 23/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:26:18,664 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:26:20,563 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:26:20,573 - experiments.base - INFO - 24/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:26:27,622 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:26:29,499 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:26:29,509 - experiments.base - INFO - 25/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:26:36,000 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:26:37,890 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:26:37,900 - experiments.base - INFO - 26/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:26:44,069 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:26:45,940 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:26:45,947 - experiments.base - INFO - 27/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:26:52,006 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:26:53,880 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:26:53,888 - experiments.base - INFO - 28/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:27:00,038 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:27:01,944 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:27:01,953 - experiments.base - INFO - 29/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:27:08,134 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:27:10,006 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:27:10,013 - experiments.base - INFO - 30/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:27:15,867 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:27:17,799 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:27:17,808 - experiments.base - INFO - 31/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:27:24,615 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:27:27,505 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:27:27,513 - experiments.base - INFO - 32/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:27:34,296 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:27:37,233 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:27:37,242 - experiments.base - INFO - 33/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:27:43,982 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:27:46,862 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:27:46,871 - experiments.base - INFO - 34/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:27:53,608 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:27:56,509 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:27:56,516 - experiments.base - INFO - 35/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:28:03,292 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:28:06,253 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:28:06,260 - experiments.base - INFO - 36/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:28:13,075 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:28:16,026 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:28:16,035 - experiments.base - INFO - 37/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:28:22,891 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:28:25,828 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:28:25,836 - experiments.base - INFO - 38/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:28:32,559 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:28:35,483 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:28:35,492 - experiments.base - INFO - 39/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:28:42,216 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:28:45,147 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:28:45,154 - experiments.base - INFO - 40/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:28:51,976 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:28:54,864 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:28:54,871 - experiments.base - INFO - 41/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:29:01,605 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:29:04,519 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:29:04,528 - experiments.base - INFO - 42/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:29:11,288 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:29:14,157 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:29:14,164 - experiments.base - INFO - 43/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:29:20,956 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:29:23,871 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:29:23,880 - experiments.base - INFO - 44/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:29:30,513 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:29:33,388 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:29:33,397 - experiments.base - INFO - 45/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:29:40,117 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:29:43,003 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:29:43,010 - experiments.base - INFO - 46/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:29:49,727 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:29:52,683 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:29:52,690 - experiments.base - INFO - 47/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:29:59,430 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:30:02,298 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:30:02,305 - experiments.base - INFO - 48/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:30:09,108 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:30:12,048 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:30:12,055 - experiments.base - INFO - 49/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:30:18,816 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:30:21,744 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:30:21,753 - experiments.base - INFO - 50/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:30:28,499 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:30:31,420 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:30:31,427 - experiments.base - INFO - 51/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:30:38,176 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:30:41,079 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:30:41,088 - experiments.base - INFO - 52/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:30:47,812 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:30:50,750 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:30:50,759 - experiments.base - INFO - 53/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:30:57,517 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:31:00,444 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:31:00,451 - experiments.base - INFO - 54/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:31:07,263 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:31:10,160 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:31:10,174 - experiments.base - INFO - 55/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:31:16,901 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:31:19,841 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:31:19,848 - experiments.base - INFO - 56/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:31:26,598 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:31:29,575 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:31:29,584 - experiments.base - INFO - 57/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:31:36,362 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:31:39,272 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:31:39,279 - experiments.base - INFO - 58/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:31:46,035 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:31:48,924 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:31:48,931 - experiments.base - INFO - 59/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:31:55,757 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:31:58,680 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:31:58,687 - experiments.base - INFO - 60/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:32:05,510 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:32:08,391 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:32:08,400 - experiments.base - INFO - 61/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:32:42,315 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:32:45,253 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:32:45,262 - experiments.base - INFO - 62/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:32:50,246 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:32:52,078 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:32:52,088 - experiments.base - INFO - 63/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:32:56,852 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:32:58,677 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:32:58,688 - experiments.base - INFO - 64/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:33:03,358 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:33:05,250 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:33:05,259 - experiments.base - INFO - 65/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:33:09,947 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:33:11,801 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:33:11,812 - experiments.base - INFO - 66/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:33:16,426 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:33:18,295 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:33:18,306 - experiments.base - INFO - 67/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:33:23,003 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:33:24,858 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:33:24,869 - experiments.base - INFO - 68/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:33:29,519 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:33:31,384 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:33:31,394 - experiments.base - INFO - 69/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:33:35,961 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:33:37,795 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:33:37,805 - experiments.base - INFO - 70/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:33:42,461 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:33:44,331 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:33:44,342 - experiments.base - INFO - 71/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:34:18,174 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:34:21,135 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:34:21,144 - experiments.base - INFO - 72/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:34:26,088 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:34:27,933 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:34:27,944 - experiments.base - INFO - 73/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:34:32,826 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:34:34,707 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:34:34,717 - experiments.base - INFO - 74/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:34:39,528 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:34:41,361 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:34:41,371 - experiments.base - INFO - 75/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:34:46,045 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:34:47,907 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:34:47,920 - experiments.base - INFO - 76/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:34:52,648 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:34:54,499 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:34:54,509 - experiments.base - INFO - 77/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:34:59,210 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:35:01,076 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:35:01,086 - experiments.base - INFO - 78/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:35:05,782 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:35:07,684 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:35:07,696 - experiments.base - INFO - 79/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:35:12,411 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:35:14,288 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:35:14,296 - experiments.base - INFO - 80/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:35:19,223 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:35:21,267 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:35:21,279 - experiments.base - INFO - 81/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:35:55,401 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:35:58,326 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:35:58,338 - experiments.base - INFO - 82/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:36:03,413 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:36:05,344 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:36:05,354 - experiments.base - INFO - 83/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:36:10,331 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:36:12,196 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:36:12,204 - experiments.base - INFO - 84/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:36:17,132 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:36:19,012 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:36:19,020 - experiments.base - INFO - 85/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:36:23,934 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:36:25,799 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:36:25,808 - experiments.base - INFO - 86/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:36:30,687 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:36:32,566 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:36:32,576 - experiments.base - INFO - 87/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:36:37,390 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:36:39,256 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:36:39,265 - experiments.base - INFO - 88/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:36:44,062 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:36:45,921 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:36:45,930 - experiments.base - INFO - 89/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:36:50,763 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:36:52,622 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:36:52,631 - experiments.base - INFO - 90/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:36:57,388 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:36:59,230 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:36:59,239 - experiments.base - INFO - 91/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:37:06,086 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:37:09,036 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:37:09,045 - experiments.base - INFO - 92/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:37:16,079 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:37:19,095 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:37:19,105 - experiments.base - INFO - 93/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:37:26,052 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:37:29,002 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:37:29,010 - experiments.base - INFO - 94/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:37:35,921 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:37:38,852 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:37:38,861 - experiments.base - INFO - 95/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:37:45,723 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:37:48,665 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:37:48,674 - experiments.base - INFO - 96/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:37:55,509 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:37:58,430 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:37:58,440 - experiments.base - INFO - 97/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:38:05,285 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:38:08,312 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:38:08,321 - experiments.base - INFO - 98/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:38:15,668 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:38:18,582 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:38:18,589 - experiments.base - INFO - 99/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:38:25,447 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:38:28,348 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:38:28,357 - experiments.base - INFO - 100/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:38:35,167 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:38:38,111 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:38:38,118 - experiments.base - INFO - 101/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:38:44,855 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:38:47,714 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:38:47,723 - experiments.base - INFO - 102/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:38:54,499 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:38:57,404 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:38:57,413 - experiments.base - INFO - 103/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:39:04,157 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:39:07,075 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:39:07,084 - experiments.base - INFO - 104/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:39:13,839 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:39:16,795 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:39:16,802 - experiments.base - INFO - 105/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:39:23,645 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:39:26,543 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:39:26,552 - experiments.base - INFO - 106/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:39:33,292 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:39:36,252 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:39:36,260 - experiments.base - INFO - 107/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:39:43,069 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:39:45,956 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:39:45,963 - experiments.base - INFO - 108/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:39:52,750 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:39:55,634 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:39:55,642 - experiments.base - INFO - 109/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:40:02,404 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:40:05,361 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:40:05,369 - experiments.base - INFO - 110/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:40:12,104 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:40:15,049 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:40:15,058 - experiments.base - INFO - 111/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:40:21,849 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:40:24,796 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:40:24,812 - experiments.base - INFO - 112/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:40:31,525 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:40:34,404 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:40:34,414 - experiments.base - INFO - 113/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:40:41,220 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:40:44,140 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:40:44,148 - experiments.base - INFO - 114/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:40:50,934 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:40:53,815 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:40:53,823 - experiments.base - INFO - 115/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:41:00,655 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:41:03,631 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:41:03,640 - experiments.base - INFO - 116/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:41:10,555 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:41:13,516 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:41:13,526 - experiments.base - INFO - 117/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:41:20,404 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:41:23,325 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:41:23,335 - experiments.base - INFO - 118/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:41:30,252 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:41:33,197 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:41:33,207 - experiments.base - INFO - 119/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:41:40,055 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:41:42,982 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:41:42,990 - experiments.base - INFO - 120/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:41:49,790 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:41:52,756 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:41:52,766 - experiments.base - INFO - 121/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:42:26,696 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:42:29,642 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:42:29,651 - experiments.base - INFO - 122/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:42:34,240 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:42:36,137 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:42:36,145 - experiments.base - INFO - 123/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:42:40,683 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:42:42,555 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:42:42,565 - experiments.base - INFO - 124/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:42:47,088 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:42:48,969 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:42:48,976 - experiments.base - INFO - 125/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:42:53,562 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:42:55,417 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:42:55,427 - experiments.base - INFO - 126/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:42:59,907 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:43:01,756 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:43:01,766 - experiments.base - INFO - 127/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:43:06,335 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:43:08,224 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:43:08,234 - experiments.base - INFO - 128/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:43:12,724 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:43:14,575 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:43:14,585 - experiments.base - INFO - 129/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:43:19,125 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:43:21,042 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:43:21,052 - experiments.base - INFO - 130/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:43:25,628 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:43:27,522 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:43:27,532 - experiments.base - INFO - 131/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:43:34,305 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:43:37,259 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:43:37,269 - experiments.base - INFO - 132/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:43:41,923 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:43:43,776 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:43:43,786 - experiments.base - INFO - 133/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:43:48,407 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:43:50,306 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:43:50,315 - experiments.base - INFO - 134/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:43:54,890 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:43:56,755 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:43:56,763 - experiments.base - INFO - 135/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:44:01,351 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:44:03,197 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:44:03,207 - experiments.base - INFO - 136/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:44:07,914 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:44:09,819 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:44:09,829 - experiments.base - INFO - 137/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:44:14,476 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:44:16,322 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:44:16,332 - experiments.base - INFO - 138/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:44:20,987 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:44:22,884 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:44:22,894 - experiments.base - INFO - 139/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:44:27,473 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:44:29,380 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:44:29,390 - experiments.base - INFO - 140/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:44:34,042 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:44:35,950 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:44:35,960 - experiments.base - INFO - 141/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:44:42,819 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:44:45,734 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:44:45,743 - experiments.base - INFO - 142/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:44:50,542 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:44:52,418 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:44:52,428 - experiments.base - INFO - 143/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:44:57,171 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:44:59,025 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:44:59,035 - experiments.base - INFO - 144/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:45:03,770 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:45:05,661 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:45:05,673 - experiments.base - INFO - 145/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:45:10,434 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:45:12,332 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:45:12,344 - experiments.base - INFO - 146/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:45:17,073 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:45:18,940 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:45:18,950 - experiments.base - INFO - 147/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:45:23,709 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:45:25,596 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:45:25,608 - experiments.base - INFO - 148/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:45:30,331 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:45:32,193 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:45:32,203 - experiments.base - INFO - 149/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:45:36,888 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:45:38,759 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:45:38,769 - experiments.base - INFO - 150/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:45:43,575 - experiments.base - INFO - Took 1000 episodes
2019-04-12 01:45:45,447 - experiments.base - INFO - reward_mean: 0.024999999999999994, reward_median: 0.025, reward_std: 6.938893903907228e-18, reward_max: 0.025, reward_min: 0.025, runs: 100
2019-04-12 01:45:45,456 - experiments.base - INFO - 151/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:45:52,323 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:45:55,259 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:45:55,270 - experiments.base - INFO - 152/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:46:02,049 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:46:04,944 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:46:04,957 - experiments.base - INFO - 153/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:46:11,783 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:46:14,697 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:46:14,707 - experiments.base - INFO - 154/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:46:21,529 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:46:24,480 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:46:24,490 - experiments.base - INFO - 155/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:46:31,385 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:46:34,312 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:46:34,323 - experiments.base - INFO - 156/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:46:41,130 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:46:44,063 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:46:44,075 - experiments.base - INFO - 157/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:46:50,868 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:46:53,763 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:46:53,775 - experiments.base - INFO - 158/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:47:00,552 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:47:03,493 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:47:03,503 - experiments.base - INFO - 159/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:47:10,309 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:47:13,224 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:47:13,234 - experiments.base - INFO - 160/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:47:20,158 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:47:23,134 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:47:23,142 - experiments.base - INFO - 161/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:47:29,928 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:47:32,839 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:47:32,855 - experiments.base - INFO - 162/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:47:39,668 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:47:42,571 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:47:42,581 - experiments.base - INFO - 163/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:47:49,420 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:47:52,388 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:47:52,395 - experiments.base - INFO - 164/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:47:59,164 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:48:02,065 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:48:02,075 - experiments.base - INFO - 165/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:48:08,931 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:48:11,891 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:48:11,901 - experiments.base - INFO - 166/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:48:18,736 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:48:21,661 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:48:21,670 - experiments.base - INFO - 167/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:48:28,443 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:48:31,398 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:48:31,407 - experiments.base - INFO - 168/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:48:38,217 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:48:41,151 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:48:41,160 - experiments.base - INFO - 169/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:48:47,957 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:48:50,940 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:48:50,950 - experiments.base - INFO - 170/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:48:57,746 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:49:00,670 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:49:00,678 - experiments.base - INFO - 171/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-12 01:49:07,500 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:49:10,401 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:49:10,408 - experiments.base - INFO - 172/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-12 01:49:17,244 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:49:20,207 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:49:20,216 - experiments.base - INFO - 173/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-12 01:49:26,992 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:49:29,897 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:49:29,905 - experiments.base - INFO - 174/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-12 01:49:36,706 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:49:39,661 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:49:39,670 - experiments.base - INFO - 175/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-12 01:49:46,441 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:49:49,328 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:49:49,335 - experiments.base - INFO - 176/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-12 01:49:56,160 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:49:59,084 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:49:59,095 - experiments.base - INFO - 177/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-12 01:50:05,895 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:50:08,786 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:50:08,796 - experiments.base - INFO - 178/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-12 01:50:15,585 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:50:18,513 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:50:18,523 - experiments.base - INFO - 179/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-12 01:50:25,342 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:50:28,253 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:50:28,262 - experiments.base - INFO - 180/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-12 01:50:35,051 - experiments.base - INFO - Took 200 episodes
2019-04-12 01:50:37,940 - experiments.base - INFO - reward_mean: 0.0, reward_median: 0.0, reward_std: 0.0, reward_max: 0.0, reward_min: 0.0, runs: 100
2019-04-12 01:50:37,956 - __main__ - INFO - {'Q': 8012, 'PI': 92, 'VI': 90}

Process finished with exit code 0




C:\Users\mcgarrah\.conda\python35\python.exe C:/Users/mcgarrah/CS-7641-assignments/assignment4/run_experiment.py --plot --verbose
2019-04-12 07:31:12,713 - __main__ - INFO - Using seed 0
2019-04-12 07:31:12,713 - __main__ - INFO - Creating MDPs
2019-04-12 07:31:12,713 - __main__ - INFO - ----------
2019-04-12 07:31:12,724 - __main__ - INFO - Mazeworld (4x4): State space: 16, Action space: 4
2019-04-12 07:31:12,724 - __main__ - INFO - Mazeworld (5x5): State space: 25, Action space: 4
2019-04-12 07:31:12,724 - __main__ - INFO - Mazeworld (11x11): State space: 121, Action space: 4
2019-04-12 07:31:12,726 - __main__ - INFO - Mazeworld (8x8): State space: 64, Action space: 4
2019-04-12 07:31:12,726 - __main__ - INFO - Mazeworld (9x9): State space: 81, Action space: 4
2019-04-12 07:31:12,726 - __main__ - INFO - Mazeworld (15x15): State space: 225, Action space: 4
2019-04-12 07:31:12,726 - __main__ - INFO - ----------
2019-04-12 07:31:12,726 - __main__ - INFO - Running experiments
2019-04-12 07:31:12,726 - __main__ - INFO - {}
2019-04-12 07:31:12,726 - __main__ - INFO - ----------
2019-04-12 07:31:12,726 - __main__ - INFO - Plotting results
2019-04-12 07:31:12,726 - experiments.plotting - INFO - Processing PI
2019-04-12 07:31:12,727 - experiments.plotting - INFO - Grid files ['output//PI\\large_mazeworld_grid.csv', 'output//PI\\medium2_mazeworld_grid.csv', 'output//PI\\medium3_mazeworld_grid.csv', 'output//PI\\medium_mazeworld_grid.csv', 'output//PI\\small_mazeworld_grid.csv', 'output//PI\\tiny_mazeworld_grid.csv']
2019-04-12 07:31:12,727 - experiments.plotting - INFO - MDP: large_mazeworld, Readable MDP: Large Mazeworld
2019-04-12 07:31:12,737 - experiments.plotting - INFO - MDP: medium2_mazeworld, Readable MDP: Medium2 Mazeworld
2019-04-12 07:31:12,746 - experiments.plotting - INFO - MDP: medium3_mazeworld, Readable MDP: Medium3 Mazeworld
2019-04-12 07:31:12,753 - experiments.plotting - INFO - MDP: medium_mazeworld, Readable MDP: Medium Mazeworld
2019-04-12 07:31:12,762 - experiments.plotting - INFO - MDP: small_mazeworld, Readable MDP: Small Mazeworld
2019-04-12 07:31:12,770 - experiments.plotting - INFO - MDP: tiny_mazeworld, Readable MDP: Tiny Mazeworld
2019-04-12 07:31:12,782 - experiments.plotting - INFO - Value file output//images/PI\medium_mazeworld_0.9_Value_Last.png, Policy File: output//images/PI\medium_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:12,782 - experiments.plotting - INFO - Value file output//images/PI\medium2_mazeworld_0.9_Value_Last.png, Policy File: output//images/PI\medium2_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:12,785 - experiments.plotting - INFO - Value file output//images/PI\small_mazeworld_0.9_Value_Last.png, Policy File: output//images/PI\small_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:12,786 - experiments.plotting - INFO - Value file output//images/PI\tiny_mazeworld_0.9_Value_Last.png, Policy File: output//images/PI\tiny_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:12,789 - experiments.plotting - INFO - Value file output//images/PI\medium3_mazeworld_0.9_Value_Last.png, Policy File: output//images/PI\medium3_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:12,790 - experiments.plotting - INFO - Value file output//images/PI\large_mazeworld_0.9_Value_Last.png, Policy File: output//images/PI\large_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:12,790 - experiments.plotting - INFO - files ['output//PI/medium_mazeworld_0.9.csv']
2019-04-12 07:31:12,790 - experiments.plotting - INFO - optimal_files ['output//PI/medium_mazeworld_0.9_optimal.csv']
2019-04-12 07:31:12,790 - experiments.plotting - INFO - episode_files []
2019-04-12 07:31:12,790 - experiments.plotting - INFO - files ['output//PI/medium2_mazeworld_0.9.csv']
2019-04-12 07:31:12,792 - experiments.plotting - INFO - optimal_files ['output//PI/medium2_mazeworld_0.9_optimal.csv']
2019-04-12 07:31:12,792 - experiments.plotting - INFO - episode_files []
2019-04-12 07:31:12,792 - experiments.plotting - INFO - files ['output//PI/small_mazeworld_0.9.csv']
2019-04-12 07:31:12,792 - experiments.plotting - INFO - optimal_files ['output//PI/small_mazeworld_0.9_optimal.csv']
2019-04-12 07:31:12,792 - experiments.plotting - INFO - episode_files []
2019-04-12 07:31:12,792 - experiments.plotting - INFO - files ['output//PI/tiny_mazeworld_0.9.csv']
2019-04-12 07:31:12,792 - experiments.plotting - INFO - optimal_files ['output//PI/tiny_mazeworld_0.9_optimal.csv']
2019-04-12 07:31:12,792 - experiments.plotting - INFO - episode_files []
2019-04-12 07:31:12,792 - experiments.plotting - INFO - files ['output//PI/medium3_mazeworld_0.9.csv']
2019-04-12 07:31:12,792 - experiments.plotting - INFO - optimal_files ['output//PI/medium3_mazeworld_0.9_optimal.csv']
2019-04-12 07:31:12,792 - experiments.plotting - INFO - episode_files []
2019-04-12 07:31:12,792 - experiments.plotting - INFO - files ['output//PI/large_mazeworld_0.9.csv']
2019-04-12 07:31:12,792 - experiments.plotting - INFO - optimal_files ['output//PI/large_mazeworld_0.9_optimal.csv']
2019-04-12 07:31:12,792 - experiments.plotting - INFO - episode_files []
2019-04-12 07:31:12,792 - experiments.plotting - INFO - Processing VI
2019-04-12 07:31:12,795 - experiments.plotting - INFO - Grid files ['output//VI\\large_mazeworld_grid.csv', 'output//VI\\medium2_mazeworld_grid.csv', 'output//VI\\medium3_mazeworld_grid.csv', 'output//VI\\medium_mazeworld_grid.csv', 'output//VI\\small_mazeworld_grid.csv', 'output//VI\\tiny_mazeworld_grid.csv']
2019-04-12 07:31:12,795 - experiments.plotting - INFO - MDP: large_mazeworld, Readable MDP: Large Mazeworld
2019-04-12 07:31:12,803 - experiments.plotting - INFO - MDP: medium2_mazeworld, Readable MDP: Medium2 Mazeworld
2019-04-12 07:31:12,813 - experiments.plotting - INFO - MDP: medium3_mazeworld, Readable MDP: Medium3 Mazeworld
2019-04-12 07:31:12,822 - experiments.plotting - INFO - MDP: medium_mazeworld, Readable MDP: Medium Mazeworld
2019-04-12 07:31:12,832 - experiments.plotting - INFO - MDP: small_mazeworld, Readable MDP: Small Mazeworld
2019-04-12 07:31:12,842 - experiments.plotting - INFO - MDP: tiny_mazeworld, Readable MDP: Tiny Mazeworld
2019-04-12 07:31:12,852 - experiments.plotting - INFO - Value file output//images/VI\medium_mazeworld_0.9_Value_Last.png, Policy File: output//images/VI\medium_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:12,854 - experiments.plotting - INFO - Value file output//images/VI\medium2_mazeworld_0.9_Value_Last.png, Policy File: output//images/VI\medium2_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:12,855 - experiments.plotting - INFO - Value file output//images/VI\small_mazeworld_0.9_Value_Last.png, Policy File: output//images/VI\small_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:12,858 - experiments.plotting - INFO - Value file output//images/VI\tiny_mazeworld_0.9_Value_Last.png, Policy File: output//images/VI\tiny_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:12,858 - experiments.plotting - INFO - Value file output//images/VI\medium3_mazeworld_0.9_Value_Last.png, Policy File: output//images/VI\medium3_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:12,861 - experiments.plotting - INFO - Value file output//images/VI\large_mazeworld_0.9_Value_Last.png, Policy File: output//images/VI\large_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:12,861 - experiments.plotting - INFO - files ['output//VI/medium_mazeworld_0.9.csv']
2019-04-12 07:31:12,861 - experiments.plotting - INFO - optimal_files ['output//VI/medium_mazeworld_0.9_optimal.csv']
2019-04-12 07:31:12,861 - experiments.plotting - INFO - episode_files []
2019-04-12 07:31:12,861 - experiments.plotting - INFO - files ['output//VI/medium2_mazeworld_0.9.csv']
2019-04-12 07:31:12,861 - experiments.plotting - INFO - optimal_files ['output//VI/medium2_mazeworld_0.9_optimal.csv']
2019-04-12 07:31:12,861 - experiments.plotting - INFO - episode_files []
2019-04-12 07:31:12,861 - experiments.plotting - INFO - files ['output//VI/small_mazeworld_0.9.csv']
2019-04-12 07:31:12,861 - experiments.plotting - INFO - optimal_files ['output//VI/small_mazeworld_0.9_optimal.csv']
2019-04-12 07:31:12,861 - experiments.plotting - INFO - episode_files []
2019-04-12 07:31:12,861 - experiments.plotting - INFO - files ['output//VI/tiny_mazeworld_0.9.csv']
2019-04-12 07:31:12,861 - experiments.plotting - INFO - optimal_files ['output//VI/tiny_mazeworld_0.9_optimal.csv']
2019-04-12 07:31:12,861 - experiments.plotting - INFO - episode_files []
2019-04-12 07:31:12,862 - experiments.plotting - INFO - files ['output//VI/medium3_mazeworld_0.9.csv']
2019-04-12 07:31:12,862 - experiments.plotting - INFO - optimal_files ['output//VI/medium3_mazeworld_0.9_optimal.csv']
2019-04-12 07:31:12,862 - experiments.plotting - INFO - episode_files []
2019-04-12 07:31:12,862 - experiments.plotting - INFO - files ['output//VI/large_mazeworld_0.9.csv']
2019-04-12 07:31:12,862 - experiments.plotting - INFO - optimal_files ['output//VI/large_mazeworld_0.9_optimal.csv']
2019-04-12 07:31:12,862 - experiments.plotting - INFO - episode_files []
2019-04-12 07:31:12,862 - experiments.plotting - INFO - Processing Q
2019-04-12 07:31:12,877 - experiments.plotting - INFO - Grid files ['output//Q\\large_mazeworld_grid.csv', 'output//Q\\medium2_mazeworld_grid.csv', 'output//Q\\medium3_mazeworld_grid.csv', 'output//Q\\medium_mazeworld_grid.csv', 'output//Q\\small_mazeworld_grid.csv', 'output//Q\\tiny_mazeworld_grid.csv']
2019-04-12 07:31:12,877 - experiments.plotting - INFO - MDP: large_mazeworld, Readable MDP: Large Mazeworld
2019-04-12 07:31:12,887 - experiments.plotting - INFO - MDP: medium2_mazeworld, Readable MDP: Medium2 Mazeworld
2019-04-12 07:31:12,894 - experiments.plotting - INFO - MDP: medium3_mazeworld, Readable MDP: Medium3 Mazeworld
2019-04-12 07:31:12,904 - experiments.plotting - INFO - MDP: medium_mazeworld, Readable MDP: Medium Mazeworld
2019-04-12 07:31:12,914 - experiments.plotting - INFO - MDP: small_mazeworld, Readable MDP: Small Mazeworld
2019-04-12 07:31:12,924 - experiments.plotting - INFO - MDP: tiny_mazeworld, Readable MDP: Tiny Mazeworld
2019-04-12 07:31:12,946 - experiments.plotting - INFO - Value file output//images/Q\medium_mazeworld_0.9_random_0.5_0.0001_0.8_Value_Last.png, Policy File: output//images/Q\medium_mazeworld_0.9_random_0.5_0.0001_0.8_Policy_Last.png
2019-04-12 07:31:12,956 - experiments.plotting - INFO - Value file output//images/Q\medium2_mazeworld_0.9_random_0.5_0.0001_0.9_Value_Last.png, Policy File: output//images/Q\medium2_mazeworld_0.9_random_0.5_0.0001_0.9_Policy_Last.png
2019-04-12 07:31:12,966 - experiments.plotting - INFO - Value file output//images/Q\small_mazeworld_0.9_random_0.5_0.0001_0.9_Value_Last.png, Policy File: output//images/Q\small_mazeworld_0.9_random_0.5_0.0001_0.9_Policy_Last.png
2019-04-12 07:31:12,976 - experiments.plotting - INFO - Value file output//images/Q\tiny_mazeworld_0.9_0_0.5_0.0001_0.9_Value_Last.png, Policy File: output//images/Q\tiny_mazeworld_0.9_0_0.5_0.0001_0.9_Policy_Last.png
2019-04-12 07:31:12,987 - experiments.plotting - INFO - Value file output//images/Q\medium3_mazeworld_0.9_random_0.5_0.0001_0.9_Value_Last.png, Policy File: output//images/Q\medium3_mazeworld_0.9_random_0.5_0.0001_0.9_Policy_Last.png
2019-04-12 07:31:12,999 - experiments.plotting - INFO - Value file output//images/Q\large_mazeworld_0.9_random_0.5_0.0001_0.9_Value_Last.png, Policy File: output//images/Q\large_mazeworld_0.9_random_0.5_0.0001_0.9_Policy_Last.png
2019-04-12 07:31:12,999 - experiments.plotting - INFO - files ['output//Q/medium_mazeworld_0.9_random_0.5_0.0001_0.8.csv']
2019-04-12 07:31:12,999 - experiments.plotting - INFO - optimal_files ['output//Q/medium_mazeworld_0.9_random_0.5_0.0001_0.8_optimal.csv']
2019-04-12 07:31:12,999 - experiments.plotting - INFO - episode_files ['output//Q/medium_mazeworld_0.9_random_0.5_0.0001_0.8_episode.csv']
2019-04-12 07:31:13,000 - experiments.plotting - INFO - files ['output//Q/medium2_mazeworld_0.9_random_0.5_0.0001_0.9.csv']
2019-04-12 07:31:13,000 - experiments.plotting - INFO - optimal_files ['output//Q/medium2_mazeworld_0.9_random_0.5_0.0001_0.9_optimal.csv']
2019-04-12 07:31:13,000 - experiments.plotting - INFO - episode_files ['output//Q/medium2_mazeworld_0.9_random_0.5_0.0001_0.9_episode.csv']
2019-04-12 07:31:13,000 - experiments.plotting - INFO - files ['output//Q/small_mazeworld_0.9_random_0.5_0.0001_0.9.csv']
2019-04-12 07:31:13,000 - experiments.plotting - INFO - optimal_files ['output//Q/small_mazeworld_0.9_random_0.5_0.0001_0.9_optimal.csv']
2019-04-12 07:31:13,000 - experiments.plotting - INFO - episode_files ['output//Q/small_mazeworld_0.9_random_0.5_0.0001_0.9_episode.csv']
2019-04-12 07:31:13,000 - experiments.plotting - INFO - files ['output//Q/tiny_mazeworld_0.9_0_0.5_0.0001_0.9.csv']
2019-04-12 07:31:13,000 - experiments.plotting - INFO - optimal_files ['output//Q/tiny_mazeworld_0.9_0_0.5_0.0001_0.9_optimal.csv']
2019-04-12 07:31:13,000 - experiments.plotting - INFO - episode_files ['output//Q/tiny_mazeworld_0.9_0_0.5_0.0001_0.9_episode.csv']
2019-04-12 07:31:13,000 - experiments.plotting - INFO - files ['output//Q/medium3_mazeworld_0.9_random_0.5_0.0001_0.9.csv']
2019-04-12 07:31:13,000 - experiments.plotting - INFO - optimal_files ['output//Q/medium3_mazeworld_0.9_random_0.5_0.0001_0.9_optimal.csv']
2019-04-12 07:31:13,000 - experiments.plotting - INFO - episode_files ['output//Q/medium3_mazeworld_0.9_random_0.5_0.0001_0.9_episode.csv']
2019-04-12 07:31:13,000 - experiments.plotting - INFO - files ['output//Q/large_mazeworld_0.9_random_0.5_0.0001_0.9.csv']
2019-04-12 07:31:13,000 - experiments.plotting - INFO - optimal_files ['output//Q/large_mazeworld_0.9_random_0.5_0.0001_0.9_optimal.csv']
2019-04-12 07:31:13,000 - experiments.plotting - INFO - episode_files ['output//Q/large_mazeworld_0.9_random_0.5_0.0001_0.9_episode.csv']
2019-04-12 07:31:13,002 - experiments.plotting - INFO - Copying output//images/PI\medium_mazeworld_0.9_Policy_Last.png to output/report//PI/medium_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:13,002 - experiments.plotting - INFO - Copying output//images/PI\medium_mazeworld_0.9_Value_Last.png to output/report//PI/medium_mazeworld_0.9_Value_Last.png
2019-04-12 07:31:13,007 - experiments.plotting - INFO - Copying output//images/PI\medium3_mazeworld_0.9_Policy_Last.png to output/report//PI/medium3_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:13,007 - experiments.plotting - INFO - Copying output//images/PI\medium3_mazeworld_0.9_Value_Last.png to output/report//PI/medium3_mazeworld_0.9_Value_Last.png
2019-04-12 07:31:13,013 - experiments.plotting - INFO - Copying output//images/PI\medium2_mazeworld_0.9_Policy_Last.png to output/report//PI/medium2_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:13,013 - experiments.plotting - INFO - Copying output//images/PI\medium2_mazeworld_0.9_Value_Last.png to output/report//PI/medium2_mazeworld_0.9_Value_Last.png
2019-04-12 07:31:13,017 - experiments.plotting - INFO - Copying output//images/PI\tiny_mazeworld_0.9_Policy_Last.png to output/report//PI/tiny_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:13,017 - experiments.plotting - INFO - Copying output//images/PI\tiny_mazeworld_0.9_Value_Last.png to output/report//PI/tiny_mazeworld_0.9_Value_Last.png
2019-04-12 07:31:13,023 - experiments.plotting - INFO - Copying output//images/PI\small_mazeworld_0.9_Policy_Last.png to output/report//PI/small_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:13,023 - experiments.plotting - INFO - Copying output//images/PI\small_mazeworld_0.9_Value_Last.png to output/report//PI/small_mazeworld_0.9_Value_Last.png
2019-04-12 07:31:13,026 - experiments.plotting - INFO - Copying output//images/PI\large_mazeworld_0.9_Policy_Last.png to output/report//PI/large_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:13,026 - experiments.plotting - INFO - Copying output//images/PI\large_mazeworld_0.9_Value_Last.png to output/report//PI/large_mazeworld_0.9_Value_Last.png
2019-04-12 07:31:13,032 - experiments.plotting - INFO - Copying output//images/VI\medium_mazeworld_0.9_Policy_Last.png to output/report//VI/medium_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:13,032 - experiments.plotting - INFO - Copying output//images/VI\medium_mazeworld_0.9_Value_Last.png to output/report//VI/medium_mazeworld_0.9_Value_Last.png
2019-04-12 07:31:13,036 - experiments.plotting - INFO - Copying output//images/VI\medium3_mazeworld_0.9_Policy_Last.png to output/report//VI/medium3_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:13,038 - experiments.plotting - INFO - Copying output//images/VI\medium3_mazeworld_0.9_Value_Last.png to output/report//VI/medium3_mazeworld_0.9_Value_Last.png
2019-04-12 07:31:13,042 - experiments.plotting - INFO - Copying output//images/VI\medium2_mazeworld_0.9_Policy_Last.png to output/report//VI/medium2_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:13,042 - experiments.plotting - INFO - Copying output//images/VI\medium2_mazeworld_0.9_Value_Last.png to output/report//VI/medium2_mazeworld_0.9_Value_Last.png
2019-04-12 07:31:13,048 - experiments.plotting - INFO - Copying output//images/VI\tiny_mazeworld_0.9_Policy_Last.png to output/report//VI/tiny_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:13,048 - experiments.plotting - INFO - Copying output//images/VI\tiny_mazeworld_0.9_Value_Last.png to output/report//VI/tiny_mazeworld_0.9_Value_Last.png
2019-04-12 07:31:13,052 - experiments.plotting - INFO - Copying output//images/VI\small_mazeworld_0.9_Policy_Last.png to output/report//VI/small_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:13,052 - experiments.plotting - INFO - Copying output//images/VI\small_mazeworld_0.9_Value_Last.png to output/report//VI/small_mazeworld_0.9_Value_Last.png
2019-04-12 07:31:13,056 - experiments.plotting - INFO - Copying output//images/VI\large_mazeworld_0.9_Policy_Last.png to output/report//VI/large_mazeworld_0.9_Policy_Last.png
2019-04-12 07:31:13,056 - experiments.plotting - INFO - Copying output//images/VI\large_mazeworld_0.9_Value_Last.png to output/report//VI/large_mazeworld_0.9_Value_Last.png
2019-04-12 07:31:13,062 - experiments.plotting - INFO - Copying output//images/Q\medium_mazeworld_0.9_random_0.5_0.0001_0.8_Policy_Last.png to output/report//Q/medium_mazeworld_0.9_random_0.5_0.0001_0.8_Policy_Last.png
2019-04-12 07:31:13,062 - experiments.plotting - INFO - Copying output//images/Q\medium_mazeworld_0.9_random_0.5_0.0001_0.8_Value_Last.png to output/report//Q/medium_mazeworld_0.9_random_0.5_0.0001_0.8_Value_Last.png
2019-04-12 07:31:13,065 - experiments.plotting - INFO - Copying output//images/Q\medium3_mazeworld_0.9_random_0.5_0.0001_0.9_Policy_Last.png to output/report//Q/medium3_mazeworld_0.9_random_0.5_0.0001_0.9_Policy_Last.png
2019-04-12 07:31:13,065 - experiments.plotting - INFO - Copying output//images/Q\medium3_mazeworld_0.9_random_0.5_0.0001_0.9_Value_Last.png to output/report//Q/medium3_mazeworld_0.9_random_0.5_0.0001_0.9_Value_Last.png
2019-04-12 07:31:13,071 - experiments.plotting - INFO - Copying output//images/Q\medium2_mazeworld_0.9_random_0.5_0.0001_0.9_Policy_Last.png to output/report//Q/medium2_mazeworld_0.9_random_0.5_0.0001_0.9_Policy_Last.png
2019-04-12 07:31:13,071 - experiments.plotting - INFO - Copying output//images/Q\medium2_mazeworld_0.9_random_0.5_0.0001_0.9_Value_Last.png to output/report//Q/medium2_mazeworld_0.9_random_0.5_0.0001_0.9_Value_Last.png
2019-04-12 07:31:13,075 - experiments.plotting - INFO - Copying output//images/Q\tiny_mazeworld_0.9_0_0.5_0.0001_0.9_Policy_Last.png to output/report//Q/tiny_mazeworld_0.9_0_0.5_0.0001_0.9_Policy_Last.png
2019-04-12 07:31:13,075 - experiments.plotting - INFO - Copying output//images/Q\tiny_mazeworld_0.9_0_0.5_0.0001_0.9_Value_Last.png to output/report//Q/tiny_mazeworld_0.9_0_0.5_0.0001_0.9_Value_Last.png
2019-04-12 07:31:13,079 - experiments.plotting - INFO - Copying output//images/Q\small_mazeworld_0.9_random_0.5_0.0001_0.9_Policy_Last.png to output/report//Q/small_mazeworld_0.9_random_0.5_0.0001_0.9_Policy_Last.png
2019-04-12 07:31:13,079 - experiments.plotting - INFO - Copying output//images/Q\small_mazeworld_0.9_random_0.5_0.0001_0.9_Value_Last.png to output/report//Q/small_mazeworld_0.9_random_0.5_0.0001_0.9_Value_Last.png
2019-04-12 07:31:13,082 - experiments.plotting - INFO - Copying output//images/Q\large_mazeworld_0.9_random_0.5_0.0001_0.9_Policy_Last.png to output/report//Q/large_mazeworld_0.9_random_0.5_0.0001_0.9_Policy_Last.png
2019-04-12 07:31:13,082 - experiments.plotting - INFO - Copying output//images/Q\large_mazeworld_0.9_random_0.5_0.0001_0.9_Value_Last.png to output/report//Q/large_mazeworld_0.9_random_0.5_0.0001_0.9_Value_Last.png
2019-04-12 07:31:13,086 - experiments.plotting - INFO - Copying file file from output//PI/medium_mazeworld_0.9.csv to output/report//PI/medium_mazeworld_0.9.csv
2019-04-12 07:31:13,092 - experiments.plotting - INFO - Copying optimal_file file from output//PI/medium_mazeworld_0.9_optimal.csv to output/report//PI/medium_mazeworld_0.9_optimal.csv
2019-04-12 07:31:13,096 - experiments.plotting - INFO - Copying file file from output//PI/medium3_mazeworld_0.9.csv to output/report//PI/medium3_mazeworld_0.9.csv
2019-04-12 07:31:13,101 - experiments.plotting - INFO - Copying optimal_file file from output//PI/medium3_mazeworld_0.9_optimal.csv to output/report//PI/medium3_mazeworld_0.9_optimal.csv
2019-04-12 07:31:13,105 - experiments.plotting - INFO - Copying file file from output//PI/medium2_mazeworld_0.9.csv to output/report//PI/medium2_mazeworld_0.9.csv
2019-04-12 07:31:13,109 - experiments.plotting - INFO - Copying optimal_file file from output//PI/medium2_mazeworld_0.9_optimal.csv to output/report//PI/medium2_mazeworld_0.9_optimal.csv
2019-04-12 07:31:13,140 - experiments.plotting - INFO - Copying file file from output//PI/tiny_mazeworld_0.9.csv to output/report//PI/tiny_mazeworld_0.9.csv
2019-04-12 07:31:13,144 - experiments.plotting - INFO - Copying optimal_file file from output//PI/tiny_mazeworld_0.9_optimal.csv to output/report//PI/tiny_mazeworld_0.9_optimal.csv
2019-04-12 07:31:13,148 - experiments.plotting - INFO - Copying file file from output//PI/small_mazeworld_0.9.csv to output/report//PI/small_mazeworld_0.9.csv
2019-04-12 07:31:13,153 - experiments.plotting - INFO - Copying optimal_file file from output//PI/small_mazeworld_0.9_optimal.csv to output/report//PI/small_mazeworld_0.9_optimal.csv
2019-04-12 07:31:13,157 - experiments.plotting - INFO - Copying file file from output//PI/large_mazeworld_0.9.csv to output/report//PI/large_mazeworld_0.9.csv
2019-04-12 07:31:13,161 - experiments.plotting - INFO - Copying optimal_file file from output//PI/large_mazeworld_0.9_optimal.csv to output/report//PI/large_mazeworld_0.9_optimal.csv
2019-04-12 07:31:13,167 - experiments.plotting - INFO - Copying file file from output//VI/medium_mazeworld_0.9.csv to output/report//VI/medium_mazeworld_0.9.csv
2019-04-12 07:31:13,173 - experiments.plotting - INFO - Copying optimal_file file from output//VI/medium_mazeworld_0.9_optimal.csv to output/report//VI/medium_mazeworld_0.9_optimal.csv
2019-04-12 07:31:13,177 - experiments.plotting - INFO - Copying file file from output//VI/medium3_mazeworld_0.9.csv to output/report//VI/medium3_mazeworld_0.9.csv
2019-04-12 07:31:13,183 - experiments.plotting - INFO - Copying optimal_file file from output//VI/medium3_mazeworld_0.9_optimal.csv to output/report//VI/medium3_mazeworld_0.9_optimal.csv
2019-04-12 07:31:13,190 - experiments.plotting - INFO - Copying file file from output//VI/medium2_mazeworld_0.9.csv to output/report//VI/medium2_mazeworld_0.9.csv
2019-04-12 07:31:13,194 - experiments.plotting - INFO - Copying optimal_file file from output//VI/medium2_mazeworld_0.9_optimal.csv to output/report//VI/medium2_mazeworld_0.9_optimal.csv
2019-04-12 07:31:13,226 - experiments.plotting - INFO - Copying file file from output//VI/tiny_mazeworld_0.9.csv to output/report//VI/tiny_mazeworld_0.9.csv
2019-04-12 07:31:13,232 - experiments.plotting - INFO - Copying optimal_file file from output//VI/tiny_mazeworld_0.9_optimal.csv to output/report//VI/tiny_mazeworld_0.9_optimal.csv
2019-04-12 07:31:13,236 - experiments.plotting - INFO - Copying file file from output//VI/small_mazeworld_0.9.csv to output/report//VI/small_mazeworld_0.9.csv
2019-04-12 07:31:13,240 - experiments.plotting - INFO - Copying optimal_file file from output//VI/small_mazeworld_0.9_optimal.csv to output/report//VI/small_mazeworld_0.9_optimal.csv
2019-04-12 07:31:13,243 - experiments.plotting - INFO - Copying file file from output//VI/large_mazeworld_0.9.csv to output/report//VI/large_mazeworld_0.9.csv
2019-04-12 07:31:13,247 - experiments.plotting - INFO - Copying optimal_file file from output//VI/large_mazeworld_0.9_optimal.csv to output/report//VI/large_mazeworld_0.9_optimal.csv
2019-04-12 07:31:13,250 - experiments.plotting - INFO - Copying file file from output//Q/medium_mazeworld_0.9_random_0.5_0.0001_0.8.csv to output/report//Q/medium_mazeworld_0.9_random_0.5_0.0001_0.8.csv
2019-04-12 07:31:13,256 - experiments.plotting - INFO - Copying optimal_file file from output//Q/medium_mazeworld_0.9_random_0.5_0.0001_0.8_optimal.csv to output/report//Q/medium_mazeworld_0.9_random_0.5_0.0001_0.8_optimal.csv
2019-04-12 07:31:13,262 - experiments.plotting - INFO - Copying episode_file file from output//Q/medium_mazeworld_0.9_random_0.5_0.0001_0.8_episode.csv to output/report//Q/medium_mazeworld_0.9_random_0.5_0.0001_0.8_episode.csv
2019-04-12 07:31:13,267 - experiments.plotting - INFO - Copying file file from output//Q/medium3_mazeworld_0.9_random_0.5_0.0001_0.9.csv to output/report//Q/medium3_mazeworld_0.9_random_0.5_0.0001_0.9.csv
2019-04-12 07:31:13,275 - experiments.plotting - INFO - Copying optimal_file file from output//Q/medium3_mazeworld_0.9_random_0.5_0.0001_0.9_optimal.csv to output/report//Q/medium3_mazeworld_0.9_random_0.5_0.0001_0.9_optimal.csv
2019-04-12 07:31:13,279 - experiments.plotting - INFO - Copying episode_file file from output//Q/medium3_mazeworld_0.9_random_0.5_0.0001_0.9_episode.csv to output/report//Q/medium3_mazeworld_0.9_random_0.5_0.0001_0.9_episode.csv
2019-04-12 07:31:13,288 - experiments.plotting - INFO - Copying file file from output//Q/medium2_mazeworld_0.9_random_0.5_0.0001_0.9.csv to output/report//Q/medium2_mazeworld_0.9_random_0.5_0.0001_0.9.csv
2019-04-12 07:31:13,295 - experiments.plotting - INFO - Copying optimal_file file from output//Q/medium2_mazeworld_0.9_random_0.5_0.0001_0.9_optimal.csv to output/report//Q/medium2_mazeworld_0.9_random_0.5_0.0001_0.9_optimal.csv
2019-04-12 07:31:13,325 - experiments.plotting - INFO - Copying episode_file file from output//Q/medium2_mazeworld_0.9_random_0.5_0.0001_0.9_episode.csv to output/report//Q/medium2_mazeworld_0.9_random_0.5_0.0001_0.9_episode.csv
2019-04-12 07:31:13,329 - experiments.plotting - INFO - Copying file file from output//Q/tiny_mazeworld_0.9_0_0.5_0.0001_0.9.csv to output/report//Q/tiny_mazeworld_0.9_0_0.5_0.0001_0.9.csv
2019-04-12 07:31:13,341 - experiments.plotting - INFO - Copying optimal_file file from output//Q/tiny_mazeworld_0.9_0_0.5_0.0001_0.9_optimal.csv to output/report//Q/tiny_mazeworld_0.9_0_0.5_0.0001_0.9_optimal.csv
2019-04-12 07:31:13,346 - experiments.plotting - INFO - Copying episode_file file from output//Q/tiny_mazeworld_0.9_0_0.5_0.0001_0.9_episode.csv to output/report//Q/tiny_mazeworld_0.9_0_0.5_0.0001_0.9_episode.csv
2019-04-12 07:31:13,352 - experiments.plotting - INFO - Copying file file from output//Q/small_mazeworld_0.9_random_0.5_0.0001_0.9.csv to output/report//Q/small_mazeworld_0.9_random_0.5_0.0001_0.9.csv
2019-04-12 07:31:13,367 - experiments.plotting - INFO - Copying optimal_file file from output//Q/small_mazeworld_0.9_random_0.5_0.0001_0.9_optimal.csv to output/report//Q/small_mazeworld_0.9_random_0.5_0.0001_0.9_optimal.csv
2019-04-12 07:31:13,368 - experiments.plotting - INFO - Copying episode_file file from output//Q/small_mazeworld_0.9_random_0.5_0.0001_0.9_episode.csv to output/report//Q/small_mazeworld_0.9_random_0.5_0.0001_0.9_episode.csv
2019-04-12 07:31:13,378 - experiments.plotting - INFO - Copying file file from output//Q/large_mazeworld_0.9_random_0.5_0.0001_0.9.csv to output/report//Q/large_mazeworld_0.9_random_0.5_0.0001_0.9.csv
2019-04-12 07:31:13,385 - experiments.plotting - INFO - Copying optimal_file file from output//Q/large_mazeworld_0.9_random_0.5_0.0001_0.9_optimal.csv to output/report//Q/large_mazeworld_0.9_random_0.5_0.0001_0.9_optimal.csv
2019-04-12 07:31:13,388 - experiments.plotting - INFO - Copying episode_file file from output//Q/large_mazeworld_0.9_random_0.5_0.0001_0.9_episode.csv to output/report//Q/large_mazeworld_0.9_random_0.5_0.0001_0.9_episode.csv
2019-04-12 07:31:17,862 - experiments.plotting - INFO - Plotting episode stats with file base output/report//Q/medium_mazeworld_{}.png
2019-04-12 07:31:18,996 - experiments.plotting - INFO - Plotting episode stats with file base output/report//Q/medium3_mazeworld_{}.png
2019-04-12 07:31:20,053 - experiments.plotting - INFO - Plotting episode stats with file base output/report//Q/medium2_mazeworld_{}.png
2019-04-12 07:31:21,178 - experiments.plotting - INFO - Plotting episode stats with file base output/report//Q/tiny_mazeworld_{}.png
2019-04-12 07:31:22,270 - experiments.plotting - INFO - Plotting episode stats with file base output/report//Q/small_mazeworld_{}.png
2019-04-12 07:31:23,335 - experiments.plotting - INFO - Plotting episode stats with file base output/report//Q/large_mazeworld_{}.png

Process finished with exit code 0
